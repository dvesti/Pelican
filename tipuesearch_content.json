{"pages":[{"url":"https://zlukfo.bitbucket.io/articles/web-application1","text":"Данная публикация начинает серию в которой описан один (мой) подход к архитектуре web-приложения некоторого типа и установки его на сервере. Что будем понимать под web-приложением Web-приложения решают очень широкий спектр задач. В этой и последующих частях статьи описана архитектура и способ реализации web-приложения, решающий задачи одного, но очень распространенного типа. Далее под web-приложением будем понимать приложение, которое: предоставляет пользователям данные по результатам запросов. Изменение данных пользователями невозможны. Запросы пользователей и получение данных по результатам запросов реализуется по протоколу http(s) Из перечисленных условий вытекают следующие условия и ограничения архитектуры приложения: объем данных, предоставляемых пользователю, большой. Иначе приложение будет малополезным - если данных мало, пользователь вряд ли по запросам сможет получить из них полезную информацию. Большой объем данных предполагает их хранение в некоторой базе. В качестве базы для хранения данных для приложения выбран postgresql . Данные могут иметь полезную информацию различной природы. Пользователю может быть интересна информация одного типа и совершенно не интересовать другие типы информации. Поэтому информацию web-приложения целесообразно разделить по категориям, будем называть их проектами. Т.е., web-приложение включает в себя один или несколько проектов (категорий информации различной природы) Информация (полезные знания), содержащаяся в данных, имеет различную ценность. Поэтому целесообразно организовать доступ к данным с несколькими уровнями привелегий. Из данного условия следуют такие требования к архитектуре приложения как: Запросы на получение данных поступают как от авторизованных пользователей (имеющих различные уровни привелений), так и от анонимных Каждый пользователь (в том числе и анонимный), выполняющий запрос на получение данных, принадлежит только одной группе в рамках одного проекта Для каждого проекта (категории информации) определены свои группы привелегий пользователей на получение данных. Для каждого проекта существует группа приведелий анонимных пользователей. Зарегистрированный пользователь может иметь доступ к нескольким проектам (категориям информации). Причем привелегии этого пользователя в различных проектах могут отличаться Протокол http(s) выполнения запросов и получения данных предполагает доступ к приложению любого , имеющего доступ в Интернет. Поэтому запросы пользователей на получение данных должны выполняться web-приложением с учетом требований безопасности и проверки привелегий получения данных. Поэтому на приложения накладываются следующие ограничения: Для каждого проекта (категории информации) задан собственный фиксированный набор запросов Для каждого запроса определены группы привелегий на его выполнение Архитектура web-приложения Исходя из перечисленных условий и требований к нашему приложению, примем следующую архитектуру. Данные хранятся в базе данных postgresql Также на базу данных возложим следуюшие задачи приложения: идентификация пользователя проверка (определение) привелегий пользователя на выполнение запроса получения данных выполнение запросов логгирование запросов пользователя к проектам Для выполнения этих задач в базе данных будет хранится схема конфигурации приложения (по умолчанию config) содержащая необходимые таблицы, функции, триггеры. И схема логов (logs), хранящая историю запросов пользователей к проектам приложения. Данные проектов (категории информации) хранятся раздельно в таблицах соотвествуюших схем. В итоге имеем базу данных - наше приложение, в которой присутствует: схема конфигурации (config) схема логов запросов одна или более схем проектов с данными различных категория Логическая схема таблиц (пункты 1,2) представлена на рисунке projects таблица, хранящая имена проектов. user_groups таблица содержит названия групп привелегий пользователей для всех проектов. Для каждого создаваемого проекта создается группа 'anonymous', описывающая привелегии для неавторизованных пользователей. Группа создается автоматически для каждого проекта, за это отвечает соотвествующий триггер users таблица хранит конфигурацию пользователей. request_sql_list - таблица хранит список доступных запросов к БД для каждого из созданных проектов. log_requests - таблица логов в схеме logs, хранящая все запросы пользователей к данным проектов Другие задачи, решаемые приложением (предварительная обработка запросов пользователей,...) описаны в последующих статьях. Здесь мы подробнее остановимся на реализации задач, решаемых на стороне базы данных. Реализация логики приложения на стороне БД Для создания приложения в базе данных и настройки его конфигурации написан python-скрипт. Его описание приведено в следующей публикации . Здесь мы остановимя на наиболее важных логических аспектах задач приложения, рещаемых на стороне базы данных. Идентификация пользователя Согласно принятой архитектуре, при добавлении нового пользователя мы привязываем его к одной группе привелегий. Но прямой ссылки на проект в связях таблиц не предусмотрено. Это необходимо для того, чтобы была возможность подключить одного пользователя (имя+пароль) к нескольким проектам. Но в данном случае на этапе добавления нового пользователя может возникнуть ситуация, когда пользователь с одинаковым именем может быть подключен к одному проекту с различными превилегиями. Чтобы исключить такую возможность создаются триггерная процедура check_user_in_projects и триггер check_user_in_projects_trigger В схему конфигурации добавлена триггерная процедура addAnonymous() и соотвествующий триггер, которые для каждого создаваемого проекта автоматически добавляет в него группу привелегий 'anonymous' (в таблицу privilege_groups). Привелегии на выполнение запроса получения данных Привелегии на получение данных по запросу определяются следующими типами ограничений а) количество запросов в единицу времени, б) интервал времени, через который могут выполняться запросы, в) объем данных, возвращаемых по результатам запросов. 2. Привелегии задаются для каждого запроса (таблица request_sql_list). Логика такая: а) пользователь (в том числе и анонимный) отправляет запрос с уникальным идентификатором (логин+пароль) и ссылкой на проект, которому адресов запрос, б) приложение (на стороне базы данных) определяет группу привелегий, которую имеет пользователь в указываемом проекте в) определяются и проверяются привелегии на выполнение данного запроса г) запрос выполняется с учетом привелегий Таким образом, для каждого запроса привелегии его выполнения удобно записать в виде json-объекта следующенй структуры (с учетом определенных выше типов ограничений) { 'group1':{'count':vc1, 'timeout':vt1, 'record':vr1 } , 'group 2 ': { 'count':vc2, 'timeout':vt2, 'record':vr2 } , ... } где ключи - имена групп, 'count' - допустимое количество запросов (в час), 'timeout' - минимальный интервал времени (сек), через который пользователь может повторить данный запрос (?? или любой другой запрос к проектцу), 'record' - максимальное число записей результатов запроса, которые может получить пользователь за одно обращение по данному запросу. В таком формате привелегии указываются (вручную) для каждого запроса (таблица request_sql_list, поле privilege). При этом сущесвует возможность неправильно указать имя группы или указать несуществующую в проекте, к которому относится запрос, группу привелегий. Чтобы исключить такие ситуации, создана триггерная процедура check_groups и триггер check_groups_trigger, возвращающая ошибку если хотя бы одна из групп привелегий для запроса была указана неверно. Отсутствие какой-либо группы (ключа) означает, что все пользолватели данной группы не имеют права выполнения данного запроса. Нулевое значение любого из параметров привелегии означает, что у данной группы отсутствуют ограничение по данному типу привелегий. Выполнение запросов Одна из идей безопасности приложения состоит в том, что пользователь не может выполнить произволный запрос к данным приложения. Для каждого проекта (категории информации) существует фиксированный (изменяемый) набор допустимых запросов. Точнее шаблонов запросов с точностью до параметров (о шаблонах читайте здесь ). Каждый запрос имеет псевдоним. Исходя из сказанного, чтобы получить нужные данные, пользователь для получения данных должен передать приложению следующую информацию - 'project' - название проекта из которого необходимо извлечь данные - 'alias' - псевдоним запроса - 'params' - словарь значений параметров запроса. - 'user' - имя пользователя - 'password': пароль (хеш пароля) Приложение, получив эту информацию выполняет ряд проверок с точки зрения безопасности и корректности запрашиваемых данных. На уровне postgresql выполняются следующие проверки проверка авторизации пользователя. Если пользователь не авторизован, ему автоматически присваивается группа привелегий 'anonymous' проверка существования запрашиваемых данных в данном проекте. В случае неудачи возвращается соотвествующее информационное сообщение. проверка привелегий пользователя на получение данных в проекте. В случае неудачи пользователю возвращается информационное сообщение. Для выполнения этих проверок в конфигурации приложения создается материализованное представление rsl_p_u_pg которое содержит всю необходимую для проверок информацию и обновляется каждый раз когда изменяются данные в любой (??? проверить) из таблиц конфигурации - пользователей, пользовательских запросов, проектов или прав привелегий. Представление содержит следующие поля: id и porject_id (дублирующее поле) - идентификатор проекта project_name - имя проекта pg_id - идентификатор группы превилегий group_name - имя группы привелегий u_id - идентификатор пользователя user_name - имя пользователя password_hash - хеш пароля пользователя rsl_id - идентификатор запроса alias - псевдоним запроса sql_query - строка запроса privilege - привелегии групп на выполнение запроса Созданное материализованное представление сохраняет результат в виде физической таблицы. Эта таблица статична в том смысле, что изменения в одной из исходных таблиц, например, добавление нового пользователя, не отразятся в нашем представлении автоматически. Чтобы увидеть изменения в представлении его нужно обновить. Для этого в конфигурации создается триггер update_view для каждой исходной таблицы, который будет выполнять обновления после каждой из операций добавления, удаления или изменения записи. И, наконец для выполнения перечисленных выше проверок пользовательского запроса создана функция getQuery select config . getQuery ( 'user' , 'password' , 'project_name' , 'alias_query' ) В случае успешного выполнения, функция вернет шаблон запроса. Как писать шаблоны запроса читайте в этой публикации Если пользователь не идентифицирован, или не имеет привелегий на выполнение запроса выполняется, функция проверяет привелегии группы 'anonymous' на выполнение этого запроса. Если группа имеет привелегии - возвращается шаблон запроса. Если нет - возвращается пустая строка. Если запрос для проекта не определен, функция вернет пустое значение Логирование запросов Логирование реализовано для анализа пользовательских запросов на предмет оптимизации и безопасности. В конфигурации приложения логируются следующие данные параметры пользователя на выполнение запроса (имя, пароль, проект, псевдоним) временная метка, пользоватеского запроса Чтобы сохранять в таблице все обращения пользователей, в функцию getQuery добавлена строка INSERT INTO config2 . log_requests ( request_time , user_name , request_alias , project , passwd ) VALUES ( now (), usr , al , project , passwd ); Таблица log_requests может быстро увеличиваться в размере, поэтому в конфигурации предусматрено патриционирование (???!!! не реализовано) . Логи запросов пользователей группируются в таблицах по месяцам. За этим следить функция add_log и триггер add_log_trigger Таблицы логов хранятся в отдельной схеме logs. Это делается из соображений безопасности. Соднанный пользователь postgresql от имени которого выполняться запросы должен иметь право создавать дочерние таблицы логов и добавлять в них записи. Создание таблиц (и других объектов) можно ограничить только на уровне схемы. Но разрешать создавать пользователю объекты в схеме config, где расположены таблицы пользователей и паролей, таблицы доступа к кпроектам и т.д. не безопасно. Поэтому логи и были вынесены в отдельную схему Чтобы выполнять быстрый поиск по логам в функции предусмотрено создание индексов. !!!?? тема индексов пока на пактике не реализована Вот и все, что хотелось рассказать об архитектуре и логике работы приоложения на стороне postgresql. В слежующей публикации рассмотрим как быстто развернуть и насроить такое приложение. -----здесь статью нужно закончить, все что ниже - полезно, но переместить в другую С таблицами логов решаются следующие задачи анализ запросов пользователя (1) по каким проектам выполняет запросы чаще всего, 2) когда был выполнен последний запрос) 1 - генерировать советы по работе с проектом, 2** - истек ли интервал времени (по привелегиям) для следующего запроса анализ запросов по проекту(какие запросы в проекте выполняются наиболее часто) - оптимизировать ресурсы для выполнения запросов !!! Нужно разобраться какой подход будет лучше - сразу выполнять поиск с указанием конкретных данных для полей. или создать представление по всем вариантам и уже по нему искать конкретные значения параметра Статистика запросов пользователя в разрезе проектов select user_name, project, count(request_alias) as c from config.log_requests group by user_name, project order by c desc 2. Время последнего запроса пользователя к конкретному проекту *** !! Этот запрос будет выполняться часто - при каждом обращении пользователя на получение данных - чтобы проверить истек ли интервал для следующего запроса согласно его привелегиям - Производительности этого запроса уделить особое внимание select user_name , project , max ( request_time ) from config . log_requests --where -- user_name='admin' -- and project='zakupki' group by user_name , project Распределение частоты выполнения запросов в разрезе проектов select request_alias , project , count ( request_alias ) as c from config . log_requests group by request_alias , project order by c desc анализ неавторизованных запросов 4.1. отлавливаем запросы от несуществующих пользователей select * from config.log_requests as l left join config.users as u using (user_name) where u.user_name is null 4.2. !!! нужно подумать как выявлять попытку взлома - многократная неправильная авторизация","tags":"Разработка","title":"Web-приложение: 1. Теория"},{"url":"https://zlukfo.bitbucket.io/articles/web-application2","text":"Предыдушая статья - Web-приложение: 1. Теория Для упрощения подготовки базы ланных postgresql нашего приложения можно использовать python-скрипт webapp-db.py. Скрипт написан как исполняемый файл и основан на работе python-библиотеки argparse. Поэтому работать со скриптом можно в двух режимах - пошаговом и пакетном. В пошаговом режиме вызов скрипта выполняется из командной строки с указанием необходимых аргументов. В пакетном режиме необходимо изменить блок скрипта main, вызывая необходимые функции с аргументами через метод argparse.parse_args(). Примеры вызова метода приведены в теле скрипта. Вызов скравки по аргументам скрипта 0. Конфигурация скрипта Конфигурация вынесена в отдельный файл - params.py, который содержит настройки подключения к серверу базы данных postgresql. Важно! Все действия скрипта выполняются от имени суперпользователя postgresql. 1. Создание приложения ./webapp-db.py -a mywebapp create -u webuser -p 12345 где: -а mywebapp - агрумент, указывающий, что мы обращаемся к приложению mywebapp create - команда, указывающая, что мы хотим создать приложение с таким именем -u webuser -p 12345 - указываем, что все пользовательские запросы будут обращаться к базе данных приложения от имени нового пользователя 'webuser', имеющего пароль '12345' На данном шаге скрипт создает: а) новую базу данных приложения, б) схемы config, logs в этой базе и в) наполняет их необходимыми объектами. Далее скрипт создает нового пользователя на сервере базы данных (если пользователя с таким именем не существует) и наделяет его правами, необходимыми только для работы с приложением. Если пользователь с указанным именем уже существует, то этот шаг пропускается, но изменяются привелегии пользователя 2. Проверка прав пользователя. Целесообразно убедиться (с точки зрения безопасности), что созданный пользователь не имеет доступа к другим базам данных и объектам, хранящимся на нашем сервере баз данных. Для этого сначала запускаем скрипт с параметрами. ./webapp-db.py view Скрипт вернет список всех баз данных сервера. Теперь, чтобы проверить права доступа нашего пользователя к объектам любой из баз данных, выполним команду ./webapp-db.py -a <database_name> view -u webuser Скрипт не предусматривает возможность последующего изменения (по необходимости) прав пользователя к базам данных и объектам. Делать это нужно будет вручную или заранее внести изменения в sql-скрипт user_role.sql - именно он управляет созданием пользователя и выделением прав для него. 3. Добавление проекта в приложение ./webapp-db.py -a mywebapp add project <project_name> Команда добавляет проект в конфигурацию приложения и создает в базе соотвествующую схему, в которой будут храниться таблицы данных проекта. 4. Добавление группы привелегий в проект Согласно логике работы приложения, описанной в первой статье , при добавлении в приложение нового проекта, скрипт автоматически создает для него группу для анонимных пользователей. Группы для авторизованных пользователей добавляются командой ./webapp-db.py -a mywebapp add group newproject -g group1 Имя группы должно быть уникально в рамках проекта 5. Добавление пользователя приложения Для корректного добавления нового пользователя приложения нужно указать три его параметра: имя, пароль, контактную электронную почту. Обязательным является только имя. Если другие параметры не заданы - скрипт выдаст предупреждение но пользователя создаст. ./webapp-db.py -a mywebapp add user newproject -g group1 -u user1 -p password -m osr@mail.ru Имя пользователя в приложении должно быть уникально для всех проектов 6. Добавление шаблона запроса в проект Здесь нужно помнить, что привелегии задаются тремя типами ограничений: количество запросов (в час), которое может выпольнить пользователь (count, число) интервал времени, через который пользовател может выполнить очередной запрос к проекту согласно установленной для него группы привелегий (timeout, число) количество записей результата запроса, возвращаемых пользователю согласно группы привелений проекта (record, число) Полная команда добавления запроса в проект выглядит так ./webapp-db.py -a mywebapp add query newproject -g group1 --alias query1 --query 'select * from table1' --rules 'count=1, timeout=2, record=3' Новыми здесь являются аргументы (--alias) - псевдоним запроса, обязательный не пустой параметр (--query) - шаблон запроса, обязательный не пустой параметр. О том, как составлять шаблоны, читать здесь (--rules) - привелегии шаблона для группы, указанной в аргументе -g При добавлении нового запроса привелегии можно не указывать. Тогда команда создаст запрос а поле привелегий оставить пустым. Это означает, что ни одна группа проекта не имеет прав на выполнение данного запроса. При определении привелегий можно задавать не все типы ограничений, а только необходимые. Для незажанных ограничений значения будут нулевые. Вниание! нудевое значени означает отсутсвик ограничения Важно понимать, что добавляемый запрос может содержать привелегии для одной или нескольких групп проекта. Но указанная выше команда добавляет новый запрос и создает привелегии только для одной группы. Для того, чтобы управлять привилегиями запроса используется все та же комманда 'add query', но с определенным набором аргументов: необходимо указать псевдоним существующего запроса (--alias), иначе будет предпринята попытка добавления нового запроса необходимо указать группу привелегий (-g) необходимо обозначить, какие действия с привелегиями (--rules) мы хотим выполнить. Здесь существуют варианты: 3.1. аргумент (--rules) не задан. Если запрос с указанным псевдонимом существует - скрипт исключает из привелегий выполнения запроса указанную группу, если таковая сущесвует ./webapp-db.py -a mywebapp add query newproject -g group1 --alias query1 3.2. аргумент привелегий задан, для указанного запроса привелегии заданной группы уже существуют. В этом случае привелегии группы обновляются. ./webapp-db.py -a mywebapp add query newproject -g group1 --alias query1 --rules 'count=1, timeout=2, record=3' Внимание! Если указывать не все типы ограничений в привелегиях - остальные будут иметь нулевые значения (ограничения отсутсвуют) 3.3. аргумент привелегий задан, для указанного запроса привелегии заданной группы еще не существуют. В этом случае привелегии группы добавляются для запроса. Вот и все. Следующая статья посвящена написанию щаблонов запросов","tags":"Разработка","title":"Web-приложение: 2. Подготавливаем базу данных"},{"url":"https://zlukfo.bitbucket.io/articles/web-application3","text":"С таблицами логов решаются следующие задачи анализ запросов пользователя (1) по каким проектам выполняет запросы чаще всего, 2) когда был выполнен последний запрос) 1 - генерировать советы по работе с проектом, 2** - истек ли интервал времени (по привелегиям) для следующего запроса анализ запросов по проекту(какие запросы в проекте выполняются наиболее часто) - оптимизировать ресурсы для выполнения запросов !!! Нужно разобраться какой подход будет лучше - сразу выполнять поиск с указанием конкретных данных для полей. или создать представление по всем вариантам и уже по нему искать конкретные значения параметра Статистика запросов пользователя в разрезе проектов select user_name, project, count(request_alias) as c from config.log_requests group by user_name, project order by c desc 2. Время последнего запроса пользователя к конкретному проекту *** !! Этот запрос будет выполняться часто - при каждом обращении пользователя на получение данных - чтобы проверить истек ли интервал для следующего запроса согласно его привелегиям - Производительности этого запроса уделить особое внимание select user_name , project , max ( request_time ) from config . log_requests --where -- user_name='admin' -- and project='zakupki' group by user_name , project Распределение частоты выполнения запросов в разрезе проектов select request_alias , project , count ( request_alias ) as c from config . log_requests group by request_alias , project order by c desc анализ неавторизованных запросов 4.1. отлавливаем запросы от несуществующих пользователей select * from config.log_requests as l left join config.users as u using (user_name) where u.user_name is null 4.2. !!! нужно подумать как выявлять попытку взлома - многократная неправильная авторизация","tags":"Разработка","title":"Web-приложение: 3. Инстументы анализа работы"},{"url":"https://zlukfo.bitbucket.io/articles/python-argparse","text":"Начиная с версии 2.7 стандартным модулем парсинга командной стоки является argparse. Создание объекта парсера import argparse parser = argparse . ArgumentParser () Вообще argparse решает две основные задачи а) парсинг аргументов вызова скрипта и б) вывод справки об этих аргументах. Справка по аргументам Выводимая в консоль справка состоит из 4-х блоков командная строка запуска скрипта (usage). По-умолчанию parse_args() анализирует все добавленные в наш объект аргументы и генерирует строку usage с учетом синтаксиса и типа этих арументов. Значение usage можно изменить при создании объекта парсера, определив параметр usage, например так parser = argparse . ArgumentParser ( usage = 'Script options \" %(prog)s \"' ) описание скрипта (description). По умолчанию не задано, задается при создании экземпляра парсера parser = argparse . ArgumentParser ( description = 'The \" %(prog)s \" is a test script' ) Далее идет перечисление аргументов, добавленных в созданный экземпляр парсера, и их описание. При этом, при выводе аргументы группируются - сначала выводится справка о позиционных аргументах, а затем - по опциональным Заключительный блок (epilog), например, с примерами использования ключей parser = argparse . ArgumentParser ( epilog = 'Sample: \" %(prog)s -h\" - view help' ) Протестировать, как будет выводиться справочная информация, можно через метод экземпляра парсера print_help() parser = argparse . ArgumentParser () parser . print_help () Argparser имеет несколько встроенных форматов, позволяющих выводить справочную информацию в различном представлении и с различной степенью детализации. Для этого используются следующие классы RawTextHelpFormatter (класс по умолчанию) ArgumentDefaultsHelpFormatter - добавляет информацию о значениях аргументов по умолчанию MetavarTypeHelpFormatter - выводит тип значения аргумента Чтобы настроить нужный формат вывода справки, создаем экземпляр парсера так parser = argparse . ArgumentParser ( formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . print_help () Добавление аргументов По умолчанию, в экземпляр созданный ArgumentParser() сразу добавляется аргумент '-h' - вывод справочной информации о скрипте и аргументах его запуска. Отключить добавление ключа '-h' можно так parser = ArgumentParser ( add_help = False ) Остальные аргументы добавляются вызовом метода add_argument(). Аргументы бывают позиционные (обязательные) и опциональные (необязательные) Имена аргументов Опциональные аргументы отличаются от позиционных наличием префикса. По умолчанию в созданном экземпляре парсера префикс - '-' (дефис). Но можно установить и другой, или несколько префиксов одновременно parser = argparse . ArgumentParser ( prefix_chars = '-#+' ) parser . add_argument ( 'p' ) parser . add_argument ( '-op' ) parser . add_argument ( '+opt' ) parser . add_argument ( '#optional' ) parser . print_help () Позиционный аргумент задается строкой - имя аргумента опциональный - строкой (которой предшествует префикс) или последовательностбю строк (через запятую) - в этом случае перечисленная последовательность имен будет считаться синонимами одного аргумента. Теоретически argparse позволяет определить аргумент таким способом parser = argparse . ArgumentParser ( prefix_chars = '-#+' ) parser . add_argument ( '-o' , '--option' , '+opt' , '+#-opti' , action = 'store' ) print parser . parse_args () Все перечисленные комбинации префикс+имя являются синонимами одного аргумента, использование любой из них в командной строке будет давать один и тот же результат. Но вряд ли кому нужен такой зоопарк. Поэтому по принятому соглашению, опциональный аргумент задается кратким именем '-o' и (необязательно) полным '--option' Псевдонимы аргументов К добавленному агрументу необходимо так-то обращаться. Делаеся это через псевдоним аргумента. Для позиционного аргумента его псевдоним совпадает с именем. Для опационального по умолчанию псевдоним формируется так: отбрасывается префикс и выбирается (из одного или более) вариант имени, имеющий максимальную длину. Но мы можем самостоятельно задать псевдоним аргумента parser = argparse . ArgumentParser () parser . add_argument ( '-a' , '--aaa' , dest = 'b' ) print parser . parse_args ([ '-a1' ]) print parser . parse_args ([ '--aaa' , '1' ]) Типы аргументов Argparse позволяет определить тип значений аргумента, задаваемого в командной строке. При парсинге аргументов командной строки проверяется соответсвие значения аргумента заданному типу. По умолчанию предполагается, что значение добавляемого аргумента имеет строковый тип (string). В качестве альтернативы мы можем задать а) целочисленный тип (int), б) число с плавающей точкой (float), в) дескриптор файла (open), или с более тонкой настройкой - argparse.FileType() (см. документацию ) parser = argparse . ArgumentParser () parser . add_argument ( '-i' , type = int ) parser . add_argument ( '-f' , type = float ) parser . add_argument ( '--file' , type = open ) p = parser . parse_args ( '-i1 -f 1.6 --file test.txt' . split ()) print p for line in p . file : print line Но самое интересное - в качестве типа значения аргумента можно задать ссылку на функцию, которая будет принимать значение как строку, преобразовывать ее к нужному типу и возвращать результат в аргумент. Эту возможность удобно использовать, например, чтобы задать аргументу несколько именованных параметров и получить результат в аргумент в виде словаря parser = argparse . ArgumentParser () parser . add_argument ( \"--params\" , type = lambda kv : dict ([ i . split ( '=' ) for i in kv . split ( \" \" )])) print parser . parse_args ([ \"--params\" , 'foo=6 bar=baz' ]) Существует еще один полезный тип значений аргумента - ограниченный список. Если заданное в командной строке значение не принадлежит этому списку - будет сгенерировано исключение. Список допустимых значений задается в параметре choices, а параметр type задает тип значения каждого элемента списка (указывается, если все элементы принадлежат одному типу) parser = argparse . ArgumentParser () parser . add_argument ( 'foo' , type = int , choices = range ( 5 , 10 )) parser . add_argument ( '-a' , choices = [ 'string' , 1 ]) print parser . parse_args ([ '6' , '-a' , 'string' ]) Действия над аргументами Параметр вызова add_argument() 'action' указывает парсеру, как необходимо обрабатывать задаваемый аргумент и его значение. По умолчанию, значение аргумента просто сохраняется (store) Если значение аргумента всегда постоянно (константа) - пользователю нет необходимости каждый раз указывать в командной строке это значение, достаточно просто указать аргумент. Чтобы парсер в данном случае корректно отработал и сохранил константу как значение аргумента, 'action' присваивается значение 'store_const' и дополнительно определяется константа parser = argparse . ArgumentParser () parser . add_argument ( '--foo' , action = 'store_const' , const = 42 ) parser . parse_args ([ '--foo' ]) Если аргумент не был указан в командной строке - в пространстве имен его значение - 'None' Параметр 'const' (и 'default' тоже) в качестве значения могут принимать ссылки на функции. Это может быть полезным, вот пример parser = argparse . ArgumentParser () parser . add_argument ( '-a' , nargs = '+' , type = int ) parser . add_argument ( '-s' , action = 'store_const' , const = sum , default = max ) p = parser . parse_args ( '-a 1 2 3' . split ()) print p . s ( p . a ) p = parser . parse_args ( '-a 1 2 3 -s' . split ()) print p . s ( p . a ) Если же аргумент по сути своей - логическое значение и нам важно только знать был ли он указан при запуске скрипта в командной строке или нет - для action задаем значение 'store_true' или 'store_false'. Для 'store_true': есди аргумент был указан в командной строке - в пространстве имен ему присваивается значение 'True', если нет - 'False'. Для 'store_false' - соответственно все наоборот Если в командной строке могут быть заданы несколько значений одного аргумента, которые необходимо объединить в список, сделать это можно задав для action значение 'append' parser = argparse . ArgumentParser () parser . add_argument ( '--foo' , action = 'append' ) parser . parse_args ( '--foo 1 --foo 2' . split ()) Реализовать такую обработку значений аргумента можно еще минимум двумя способами 1) определив функцию-обработчик в type добавляемого аргумента, 2) указав количесво значений nargs для аргумента (см. описание соответсвующих параметров) Если существует аргумент, значение для которого не предусмотрено и необходимо посчитать сколько раз этот аргумент был указан в командной строке - используем значение 'count' parser = argparse . ArgumentParser () parser . add_argument ( '--verbose' , '-v' , action = 'count' ) print parser . parse_args ([ '--verbose' , '-v' , '-vvv' ]) Данное действие может быть полезно, если аргумент отождествляет, например, уровень. Тогда '-v' соотвествует первому уровню, '-vv' - второму и так далее. Сущесвуют еще виды действий 'help' и 'version', посмотреть их описание можно в документации. Также, есди описанных типов действия недостаточно для описания поведения аргумента, можно создать собственное дейсвие, определив его через класс (см. документацию) Количество значений опционального аргумента По умолчанию количество значений аргумента равно 1. Но можно задать любое, определив их число через параметр nargs при добавлении аргумента. Если количество значений для аргумента известно и фиксированно - в nargs указываем соответствующее число. Если аргумент может быть а) указан в комадной строке с одним значением, б) указан без значения, в) не указан вовсе - задаем nargs='?' parser = argparse . ArgumentParser () parser . add_argument ( '-f' , nargs = '?' , const = 'c' , default = 'd' ) print parser . parse_args () print parser . parse_args ([ '-f' ]) print parser . parse_args ([ '-f7' ]) Если случай аналогичен случаю выше, но количетво аргументов может быть один и более - задаем nargs='*'. В данном случае применение 'const' недопустимо. Еще один случай - если число значений аргумента должно быть не менее одного . В этом случае используется значение nargs='+' Указание обязательных аргументов Параметр 'required=True' - удобное дополнение, чтобы указать, что аргумент является обязательным для определения в командной строке. В принципе, реализовать данную проверку можно и без этого параметра, но он облегчает проверку Получение аргументов и их значений Чтобы при запуске скрипта получить набор заданных в командной строке аргументов и их значений, используется метод parse_args(). Ранее он неоднократно встречался в примерах. Метод создает экземпляр класса Namespace, который включает все заданные в командной строке аргументы (позиционные и опциональные) и их значения. Доступ к значению аргумента (если он был задан в командной строке) осуществляется по его псевдониму parser = argparse . ArgumentParser () parser . add_argument ( '-a' ) p = parser . parse_args () print p . a Получить значение всех аргументов (в том числе и не заданных в командной строке) можно через метод _get_kwargs() parser = argparse . ArgumentParser () parser . add_argument ( '-a' ) parser . add_argument ( '-b' ) p = parser . parse_args () print p . _get_kwargs () Значения незаданных аргументов будут 'None'. Протестировать из скрипта как будут распознаваться заданные аргументы и их значения можно так parser = argparse . ArgumentParser () parser . add_argument ( '-a' ) parser . add_argument ( '-b' ) parser . add_argument ( 'с' ) p = parser . parse_args ([ 'c' , '-a' , '1' , '-bone' ]) print p Здесь несколько важных моментов работы парсера аргументы в командной строке можно задавать в любой последовательности значения опициональных аргументов можно НЕ отделять от имени аргумента пробелом И вообще, опциональные аргументы в командной строке можно определять несколькими способами 'test.py -a 1' 'test.py -a1' 'test.py -a=1' И еще метод parse_args() позволяет не полностью указывать имя аргумента, а только первую его часть. Если при этом не возникает конфликта имен . Например parser = argparse . ArgumentParser () parser . add_argument ( '-barcode' ) parser . add_argument ( '-banner' ) p = parser . parse_args () print p Вызов 'test.py -bar 1' правильно определит какой апгумент был задан в командной строке. А 'test.py -ba 1' приведет к ошибке конфликта имен Еще Namespace может быть легко преобразован в словарь parser = argparse . ArgumentParser () parser . add_argument ( '-a' , nargs = '+' , type = int ) p = parser . parse_args ( '-a 1 2 3' . split ()) print vars ( p ) Аргументы из файла Argparse позволяет передавать скрипту значения аргументов из файлов. Это полезно когда а) при запуске скрипта в командную строку необходимо вводить большое количество аргументов, что неудобно, б) можно использовать как аналог конфигурационных файлов запуска скрипта с различными устойчивыми наборами параметров. Загрузка аргументов из файла выполняется так parser = argparse . ArgumentParser ( fromfile_prefix_chars = '#' ) parser . add_argument ( '-f' , nargs = 2 ) parser . add_argument ( '--bar' ) parser . add_argument ( 'b' ) p = parser . parse_args ([ '#arg.test' ]) print p При создании экземпляра парсера необходимо определить односимвольный префикс. Он будет указывать парсеру, что после этого префикса идет имя файла (а не очередной аргумент) из которого нужно получить набор аргументов для запуска скрипта. Далее, в файле можно перечислять только те аргументы, которые добавлены в экземпляр парсера. правильно. Правило составления файла с аргументами следующее: сначала идет псевдоним опционального аргумента (со всеми необходимыми префиксами), затем на следующей строке - его значение (если предусмотрено). Если аргумент предполагает несколько значений каждый указывается в новой строке. В остальном правила указания аргументов в файле (в том числе и их порядок) таакой же как и в командной строке. Теперь при запуске скрипта достаточно только указать имя файла с аргументами. Кстати ничто не мешает нам задавать аргументы параллельно в файле и командной строке. В данном случае парсер работает по следующему правилу: значение аргумента будет таким, который был задан последним. Тоесть если сначала указать имя файла с аргументами, а затем определить аргумент в командной строке, то аргументу будет присвоено значение из командной строки. И наоборот И еще пара моментов. Описанные правила указания аргументов в файле определяются работой парсера по умолчанию. Вместе с тем argparse позволяет создать собственный парсер, задающий другие правила. Соответственно и порядок указания аргументов в файле изменится. Подробнее - в документации Чтобы избежать исключение парсера, вызванного тем, что в файле определены аргументы не добавленные в парсер, в скрипте можно предусмотреть проверку. Делается это при помощи метода parse_known_args(). Например так parser = argparse . ArgumentParser ( fromfile_prefix_chars = '#' ) parser . add_argument ( '-a' ) parser . add_argument ( '-b' ) p = parser . parse_known_args ([ '#arg.test' ]) print p Метод возвращает кортеж, первый элемент которого - Namespace распознанных (добавленных в парсер) аргументов и их значений, которые были заданы при вызове скрипта (в командной с троке или в файле). Второй элемент - список нераспознанных аргументов и их значений Команды Их идея в следующем. Запускаемый скрипт может выполнять несколько задач, каждая из которых а) может быть описана отдельной командой, б) каждая команда имеет собственный набор аргументов. При этом, наборы аргументов различны, слабо пересекаются друг с другом или не пересекаются вовсе. Если это не так (все команды имеют одинаковой (почти) набор аргументов), то городить огород с выделением команд скрипта не имеет смысла. Приведем пример - скрипт работы с базой данных, который должен выполнять задачи: добавление записи в таблицу, задача 'add' просмотр записей таблицы по фильтру, задача 'view'. Очевидно, что сначала нужно подключиться к бд. Значит необходим аргумент, задающий параметры подключения, назовем его '-p'. Этот аргумент необходим для всех задач. Для задачи 'add' необходим аргумент '-d', задающий данные для полей добавляемой записи. А для задачи 'view' - аргумент, определяющий фильтр '-f'. Итого, для задач имеем набор общих аргументов ['-p'] и набор не пересекающихся ['-d'] и ['-f']. Реализация примерно такая parser = argparse . ArgumentParser () parser . add_argument ( '-p' , required = True , help = 'Connection params (for all commands)' ) # создаем подпарсер комманд пользователя. Внимание !! в парсер можно добавить только ОДИН подпарсер user_subparsers = parser . add_subparsers ( help = 'Script commands for user' ) # добавляем команды и аргументы для каждой из них parser_add = user_subparsers . add_parser ( 'add' , help = \"Discribe for command 'add'\" ) parser_add . add_argument ( '-d' , help = 'add help' ) parser_view = user_subparsers . add_parser ( 'view' , help = \"Discribe for command 'view'\" ) parser_view . add_argument ( '-f' , help = 'view help' ) parser . print_help () print parser . parse_args ([ '-p' , 'j' , 'add' ]) Команды являются обязательными . Определив в коде скрипта, что мы будем использовать команды, при вызове в командной строке мы обязаны указывать одну из них Что нам дает применение команд - Более ясное понимание выполняемых действий. Если исключить команды у нас получится один общий набор атрибутов ['-p', '-d', '-f']. Причем для одного действия обязательной будет только часть комманд. Для другого - другая часть. В итоге, у пользователя скрипта может возникнуть непонимание, какое сочетание аргументов обязательно использовать, чтобы выполнить конкретное дейсвие. Команды помогают сориентироваться в наборе необходимых аргументов. А вывод справки print parser . parse_args ([ 'add' , '-h' ]) помогает понять какие аргументы необходимы для выполнения действия. В итоге, используя команды, мы можем упростить код нашего скрипта. Поясним, используя пример выше Если мы реализуем запуск скрипта без команд, то чтобы определить, какое дейсвие хочет выполнить пользователь, необходимо проверить (if...else), какие документы он задал, и уже после этого запускать соотвествующий код. Но если мы реализуем выполнение команд в скрипте, то для каждой из них можем задать функцию которая будет выполнять необходимый код, а задание проверки необходимых параметров возложить на argparse. Этот прием реализуется с помощью метода set_defaults() Задание аргументов по умолчанию (set_defaults()) Метод задает для парсера (или команды) арументы и их значения, которые будут добавлены в Namespace автоматически, без необходимости их определения пользователем. В качестве значения аргумента по умолчанию мы можем добавить и функцию, которая будет выполнять необходимые действия. Таким образом определение и выполнение намерений пользователя с помощью команд может выглядеть так(в качестве примера напишем скрипт решаюший две задачи: 1) возведение числа в степень и 2) получение натурального логарифма из числа. Для обоих задач существует обший обязательный аргумент - число. Кроме того, для задачи (1) должен быть определен дополниельный обязательный аргумент - степень (число с плавающей точкой). Для задачи (2) нужен аргумент основание логарифма). def pow ( args ): return args . d ** args . s import math def log ( args ): return math . log ( args . d , args . b ) if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '-d' , required = True , type = int ) subparsers = parser . add_subparsers () parser_pow = subparsers . add_parser ( 'pow' ) parser_pow . add_argument ( '-s' , required = True , type = float ) parser_pow . set_defaults ( f = pow ) parser_log = subparsers . add_parser ( 'log' ) parser_log . add_argument ( '-b' , type = float ) parser_log . set_defaults ( f = log ) p = parser . parse_args ([ '-d 2' , 'pow' , '-s 3' ]) print p . f ( p ) p = parser . parse_args ([ '-d 2' , 'log' , '-b 3' ]) print p . f ( p ) Т.е., применяя команды отпрадает необходимость писать проверки на необходимые аргументы - эту задачу выполняет argparse В статье рассмотрены основные (не все) водзможности использования argparse. Подробнее - в документации (англ.)","tags":"разработка","title":"Python: приемы парсинга аргументов командной строки"},{"url":"https://zlukfo.bitbucket.io/articles/opendata","text":"RSS каналы Агентства новостей Regnum https://regnum.ru/rss/news Блумберг https://www.bloomberg.com/feeds/podcasts/etf_report.xml Открытое правительство госзакупки http://zakupki.gov.ru/epz/order/extendedsearch/rss Работа биржа фриланса https://freelance.ru/rss/index hh https://hh.ru/search/vacancy/rss?&order_by=salary_desc&&specialization=1 http://www.it-rabota.ru/resume.xml Строительство http://neagent.info/realty/rss/ http://bc-lars.ru/rss.xml http://www.newmetr.ru/analitika-obozrenie.feed?type=rss Рынок Курсы валют Ежедневно http://www.cbr-xml-daily.ru/ Ежедневно (европейский банк) http://www.ecb.europa.eu/stats/eurofxref/eurofxref-daily.xml Архив https://www.cbr.ru/scripts/Root.asp?PrtId=SXML","tags":"Анализ данных","title":"Открытые источники данных (для парсинга)"},{"url":"https://zlukfo.bitbucket.io/articles/cycl-parsing","text":"В статье описан один вариант реализации цикдического парсера на python. Что вкладывается в понятие циклический парсер и каковы его особенности, можно прочитать в статье . Если коротко - циклический парсер извлекает данные по фиксированному адресу (с динамической информацией), образаясь к нему циклически (через заданные промежутки времени). Также помним, что html - подмножество языка xml, значит парсер будет подходить и для html страниц с данными. При написании парсинга использовался скрипт xmlIterParser, прочитать о нем можно в этой статье Код парсера можно найти на bitbucket Структура парсера Важное замечание! Описываемый парсер - это шаблон, некоторая обвязка для создания парсеров различных источников данных (динамических). Источники могут иметь разную xml структуру данных - набор узлов хранящих необходимые данные и пути доступа к этим узлам. Поэтому настройка парсера под конкретный проект предполагает написание некоторого количества кода на python. Тем не менее, парсер разрабатывается с таким учетом, чтобы написание кода было максимально легким. Подробности читаем ниже. Парсер состоит из python-скриптов cycl-parsser.py - скрипт запуска парсера (исполняемый файл) daemonize.py - скрипт, содержащий функцию, переводящий работу парсера в фоновый режим xmlIterParser.py - скрипт, включает методы класса, выполняющий анализ структуры источника и непосредственное извлечение данных из источника. utilFunc.py - скрипт, включающий функции-утилиты В каталоге _template находятся скрипты-шаблоны для создания проектов парсера для конкретных xml источников Парсер написан для сохранения извлекаемых данных в бд postgresql. Как работать с парсером Парсер - консольное приложение, запускается из командной строки. Краткая справка по параметрам запуска доступна стандартно ./cycl-parser.py -h Алгоритм применения парсера следуюший 1. Создание проекта Выполняем команду ./cycl-parser.py -c project_name В каталоге парсера создается директория project_name в которую копируются файлы настроек проекта. 2. Получем структуру источника данных (опционально) Данный шаг выполняется, если точно не знаем какие данные и из каких узлов источника будем извлекать. Чтобы получить структуру источника - нужно указать парсеру его адрес и тип. Эти параметры задаются в project_name/params.py - SOURCEURL, SOURCETYPE. Запускаем команду ./cycl-parser.py -t [ png | svg ] project_name Команда обратится к источнику по адресу SOURCEURL и сохранит в каталоге проекта project_name структуру источника. Формат сохраненения структуры выбирается из допустимых значений png или svg Сохраненный графический файл помогает понять, какие узлы источника содержат данные, необходимые для извлечения и указать список xpath путей к этим узлам в переменной XPATH2SOURCEDATA 3. Восстанавливаем пути к элементам данных (опционально) Если структура источника данных незнакома, иерархия структуры, созданная на шаге 2, может не дать полного понимания в каких узлах источника содержится необходимая информация. В данном случае поможет следующий функционал парсера. Команда восстановления xpath пути к узлу по его характеристикам. ./cycl-parser.py -p 'tagname=title' project_name Команда по имени тега (узла) вернет массив xpath путей (массив - потому что на разных ветках иерархии могут находиться узлы с одинаковым именем). Путь узлам можно искать не только по его имени, но и по имени атрибута и его значению (если они присутсвуют). Эти три фильтра (имя узла, имя атрибута, значение атрибута) можно комбинировать при вызове команды в любой последовательности. Ни один из фильтров команды не является обязательным. Полный формат вызова команды парсера такой ./cycl-parser.py -p 'tagname=val1, attrname=val2, attrval=val3' project_name Полученные нужные пути сохраняем в настройках проекта project_name/params.py в переменную XPATH2SOURCEDATA. Теперь мы можем протестировать какие данные будет извлекать парсер ./cycl-parser.py -x project_name Команда вернет первые несколько строк, извлеченные из источника данных в формате <имя_узла>:<значение>. Внимание !!! Нам важно четко понимать, в каком формате парсер будет возвращать извлекаемве данные. Непременное правило - парсер должен возвращать список, каждый элемент которого содержит ровно одну запись. Эти знания необходимы для написания функции насстановки данных (см. ниже шаг 6) 4. Создаем таблицу postgresql Данный шаг не относится к работе парсера, выполняется вручную. В созданную таблицу парсер будет сохранять извлеченные данные. Предполагается, что читатель знает как создавать таблицы в postgresql. 5. Настройка параметров проекта Все параметры проекта хранятся в файле params.py в каталоге проекта. Описание параметров - там же. 6. Написание функции расстановки данных Это единственный шаг, требующий навыков программирования на python. Он сводится к написанию функции, которая будет расставлять данные каждой извлеченной записи в соответсвии с порядком полей таблицы бд, определенной в параметре настройки FIELDSNAME. Шаблон функции и ее описание хранится в файле parser.py в каталоге проекта. Вот пример функции для проекта, извлекающего данные из стандартной rss ленты новостей # файл params.py FIELDSNAME = ( \"link\" , \"title\" , \"description\" , \"pubDate\" , \"category\" ) # файл parser.py def parseData ( d ): data = [ 'None' ] * len ( FIELDSNAME ) for k , v in d . items (): v = v . replace ( ' \\n ' , '' ) if k in FIELDSNAME : data [ FIELDSNAME . index ( k )] = v return data По-моему достаточно просто 7. Запуск парсера на извлечение данных ./cycl-parser.py [ -d -i val1 val2 ] project_name Параметры, указанные в скобках могут быть опущены (не являются обязательными). Параметр -d запускает парсер в фоновом режиме, параметр -i с обязательным указанием двух целочисленных значений задает периодичность (в сек.), с которой парсер будет обращаться к источнику - val2 и период (в сек.) через который извлеченные данные будут переноситься из csv файла-буфера в таблицу бд. После переноса данных в бд файла-буфер очищается. Еще один режим работы парсера определяется параметром HASHKEY в файле настроек проекта. По умолчанию значение параметра - пустрой список. Это означает, что сохраняться будут все извлекаемые записи, без проверки их уникальности. Поэтому, если параметром -i задать достаточно короткий период обращения парсера к источнику данных (до того, как данные будут успевать обновиться полностью), то таблица будет содержать записи-дубликаты. Чтобы избежать этого - необходимо из кортежа полей таблицы (FIELDSNAME) в списке переменной HASHKEY указать одно или несколько полей, которые будут определять уникальность каждой записи. Тоесть, проверка уникальности выполняется не на уровне postgresql, а на уровне парсера. Принцип работы парсера В ходе работы парсер выполняет следующие действия подключается к указанной базе (единожды, при запуске парсера) извлекает данные из источника и сохраняет их в csv файл При следующем обращении парсер вновь извлекает данные. Если задана проверка уникальности, то прежде чем добавить очередной объект (запись), парсер проверяет ее на уникальность. Неуникальные записи игнорируются. При наступлении времени сохранить извлеченные данные в базу, парсер выполняет эту операцию и затем очищает csv файл. цикл повторяется В ходе работы парсер по заданному в PATH2FILE пути создает следующие файлы файл логов - в нем сохраняется время обращения к источнику и количество новых записей, добавленных в csv файл сам csv файл файл ошибок - хранит время ошибки и ее описание файл хешей уникальных записей.","tags":"Анализ текста","title":"Python: Реализация циклического парсера xml источников в Интернет"},{"url":"https://zlukfo.bitbucket.io/articles/get-string-from-end-file","text":"Необходимость возникла при решении одной практической задачи: перед загрузкой из скрипта python новых записей в БД проверялась уникальность каждой записи. Проверка проводилась по хешам, которые списком хралились в текстовом файле. Природа записей была такова, что вероятность того, что проверяемая запись уже существует в БД уменьшалась со временем. Очевидное решение быстрой проверки - сохранять хеш очередной записи (в том случае, если она уникальна) в начале файла. Однако python не дает простого решения для этой операции - такое сохранение возможно либо через предварительную загрузку списка в память, либо через соединение нескольких файлов, по умолчанию запись добавляется либо в конец файла, либо в произвольное место, затирая существующую информацию. В итоге возникла идея написать функцию, возвращающую строки, начиная с конца файла. Сразу замечу, что функция писалась под конкретную задачу, поэтому имеет ограничение - длины всех строк имеют одинаковый размер (длина хеша). Вот эта функция def getStringFromEndFile ( filename , string_len , delimiter_line_len = 1 , emty_line_in_end = False ): with codecs . open ( filename , 'r' , 'utf-8' ) as hk : if emty_line_in_end : hk . seek ( 0 , 2 ) else : hk . seek ( 1 , 2 ) while hk . tell () > string_len : hk . seek ( - ( string_len + delimiter_line_len ), 1 ) yield hk . read ( string_len ) hk . seek ( - string_len , 1 ) параметры: filename - имя файла, string_len - длина строки, delimiter_line_len - длина разделителя: новая строка, любая последовательность символов, их комбинации, не важно что, лишь бы все разделитили имели одинаковую длину emty_line_in_end - заканчивается ли файл пустой строкой (True / False) пример использования: for line in getStringFromEndFile ( '/tmp/hash.key' , 3 ): print line","tags":"Программирование на python","title":"Python: получение строк из файла в обратном порядке"},{"url":"https://zlukfo.bitbucket.io/articles/postgresql-fts-practic","text":"В этой публикации описывается реализация упрощенной но вполне прикладной задачи полнотекстового поиска по базе исходных текстов. Под прикладной задачей здесь понимается конечно не поиск максимально похожей последовательности символов, составляющей слова и фрагменты текста. Такой поиск малоинформативен и вполне может быть реализован без инструментов fts. Мы будем искать в базе текстов некоторые сущности и извлекать из текстов полезную информацию. Сформулируем следующее допущение Каждый текст в базе данных может содержать объект, выраженный одущевленным существительным и событие, выраженное глаголом . Это очень упрощенное допущение тем не менее позволяет нам а) сформулировать практическую задачу поиска полезной информации в большом массиве текстов, и б) разобраться в настройках полнотекстрового поиска в postgresql Задача (см. допущение выше) По пользовательскому запросу в базе исходнфх текстов найти наиболее релевантные записи, содержащие искомый объект(-ы) и (или) связанное с ним событие Исходные данные В качестве исходных данных возьмем новостную тематику текстов. Тестовую выборку по данной тематике на различных языках можно взять здесь . В нашем случае тестовая выборка состоит из 1 000 000 записей. Каждая запись представляет одно предложение (здесь это не принципиально, текст (запись) может состоять из нескольких предложений). Данную выборку будем считать репрезентативной, т.е. объективно описывающей характеристики и особенности текстов похожей тематики (новостей на русском языке). Решение Импорт данных в БД Для импорта скачанных из указанного источника данных можно использовать такой скрипт # -*- coding: utf-8 -*- import codecs import psycopg2 if __name__ == '__main__' : dbname = \"{{dbname}}\" conn = psycopg2 . connect ( \"dbname=' %s ' user='{{username}}' password='{{userpassword}}'\" % ( dbname ,)) conn . autocommit = True cur = conn . cursor () with codecs . open ( '{{filename}}' , 'r' , 'utf-8' ) as fr : for line in fr : q = \"INSERT INTO {{schema}}.news (sent) VALUES ( %s );\" cur . execute ( q , ( line ,)) conn . close () 1. Создание словарей Поскольку в нашей задаче важно выделять объекты (одущевленные существительные) и события (глаголы), сгенерируем соответсвующие словари. Для генерации ispell-словарей используем материалы этой статьи . В итоге получим 5 файлов словарь людей и фамилий словарь одушивленных предметов словарь глаголов (совершенных и несовершенных) Cоздаем в бд postgresql необходимые словари. -- словарь имем CREATE TEXT SEARCH DICTIONARY {{ schema_name }} . noun_name ( TEMPLATE = ispell , dictfile = 'noun_name' , afffile = 'ru' , stopwords = 'russian' ); -- словарь фамилий CREATE TEXT SEARCH DICTIONARY {{ schema_name }} . noun_surn ( TEMPLATE = ispell , dictfile = 'noun_surn' , afffile = 'ru' , stopwords = 'russian' ); -- словарь одушевленных существительных CREATE TEXT SEARCH DICTIONARY {{ schema_name }} . noun_anim ( TEMPLATE = ispell , dictfile = 'noun_anim' , afffile = 'ru' , stopwords = 'russian' ); -- словарь глаголов (совершенных) CREATE TEXT SEARCH DICTIONARY {{ schema_name }} . verb_perf ( TEMPLATE = ispell , dictfile = 'verb_perf' , afffile = 'ru' , stopwords = 'russian' ); -- словарь глаголов (несовершенных) CREATE TEXT SEARCH DICTIONARY {{ schema_name }} . verb_impf ( TEMPLATE = ispell , dictfile = 'verb_impf' , afffile = 'ru' , stopwords = 'russian' ); Этап 3. Создаем конфигурацию. CREATE TEXT SEARCH CONFIGURATION {{ schema_name }} . noun_verb ( PARSER = default ); ALTER TEXT SEARCH CONFIGURATION {{ schema_name }} . noun_verb ADD MAPPING FOR word WITH {{ schema_name }} . noun_name , {{ schema_name }} . noun_surn , {{ schema_name }} . verb_perf , {{ schema_name }} . verb_impf , {{ schema_name }} . noun_anim Обратите внимание - словарь одушивленных существительных мы поместили в самом конце цепочки. Это сделано для того, чтобы наша конфигурация распознавала одушивленные объекты только в том случае если они не распознаны словарями имен и фамилий. Поскольку по смыслу формулировки задачи и тематике текстов запросы типа \"Обама встретился\" будут преобладать над \"ежик прибежал\", большинство искомых объектов будут распознаваться словарями имен и фамилий, а такая конфигурация сократит время поиска. Тестируем select * from to_tsvector ( '{{schema_name}}.noun_verb' , 'Встретились с ОБАМОЙ вчера ранним вечером в 19' ) Тест показывает преимущества такой конфигурации - в tsvector попадают только существительные (одушивленные!!!) и глаголы - остальные слова исходного текста игнорируются. Это позволяет сократить длину вектора и тем самым повысить скорость поиска. Есть и еще одно преимущество - исключение из tsvector незначимых для поиска (в данной постановке задачи) слов оказывает положительный эффект при оценке релевантности поиска. Более подробно данный вопрос раскрыт далее. Теперь можем выполнять поиск select * from {{ schema_name }} . news where to_tsvector ( '{{schema_name}}.noun_infn' , sent ) @@ plainto_tsquery ( '{{schema_name}}.noun_infn' , 'МЕДВЕДЕВ посетил' ) Этап 4. Настройка производительности. Конечно такой поиск малоэффективный. Потому что медленный. Чтобы сократить время поиска прежде всего создадим индекс. CREATE INDEX gin_noun_verb ON {{ schema_name }} . news USING GIN ( to_tsvector ( '{{schema_name}}.noun_verb' , sent )); Совсем другое дело. Скорость поиска по запросу выше увеличилась в десятки раз. Этап 5. Настраиваем релевантность результатов запроса Релевантность - это мера близости по смыслу результатов поиска поисковому запросу. Формулировка такая нечеткая, потому что конкретное значение этой меры опрелеляется разработчиком для каждого прикладного проекта отдельно. При постановке нашей задачи мы только определили, что результаты поиска должны быть отсортированы по релевантности, но саму меру релевантности не определили. Пора это исправить. Применительно к нашей задаче будем полагать, что наиболее релевантными являются те тексты, в которых упоминается объект(-ы) и действия (события), связанные именно с этим объектом . Например, во фразе \"Обама встретился с Медведевым, который вчера общался с Меркель.\" имеем следующие связки объект-событие (Обама-встретился), (Меркель-общалась), (Медведев-встретился), (Медведев-общался). Если в поисковом запросе будет указано \"Обама встречался\", то фраза выше должна иметь высокую релевантность, потому что соответсвует смыслу запроса. Но если поисковый запрос будет составлен как \"Обама общался\" то он будет отражаться в результатах поиска (во фразе присутсвует объект \"Обама\" и событие \"общался\"), но релевантность в данном случае должна быть существенно ниже, поскольку событие \"общался\" не относятся к объекту \"Обама\". Чтобы сопоставить объекты и относящиеся к ним события, по-хорошему, необходимо предварительно выполнить синтаксический разбор исходной фразы. Но данная задача не тривиальна и ее описание выходит далеко за рамки статьи. Поэтому, далее мы примем одно интуитивное допушение, которое для некоторых типов текстов может быть весьма эффективным (его обоснование или опровержение оставим для других статей). Связанные по смыслу сущности (объекты и события) в тексте стремятся располагаться как можно ближе друг к другу. Приняв такое допущение мы можем весьма эффективно использовать функцию ts_rank_cd() для оценки релевантности каждого результата запроса. И здесь наша конфигудация noun_verb играет не последнюю роль. Рассмотрим следующий пример select ts_rank_cd ( to_tsvector ( 'corpora.noun_verb' , 'Обама встретился с Медведевым' ), plainto_tsquery ( 'corpora.noun_verb' , 'обама встретился' )) Оценка релевантности = 0.1. Обратим внимание, что объект и событие располагаются рядом. Поэтому оценка имеет максимальное значение (в шкале по-умолчанию. О том, как рассчитывается оценка и диапазоне изменения ее значений можно прочитать в статье ). Теперь немного изменим исходный текст select ts_rank_cd ( to_tsvector ( 'corpora.noun_verb' , 'Обама вчера встретился с Медведевым' ), plainto_tsquery ( 'corpora.noun_verb' , 'обама встретился' )) Оценка релевантности не изменится = 0.1. Это вполне соответсвует условиям нашей задачи. Нас интересуют только объекты и связанные с ними события, другая уточняющая информация (время наступления события) для нас несущественна. Поэтому с точки зрения созданной нами конфигурации исходные фразыв первом и втором запросах являются равнозначными, а результат запроса по ним имеет одинаковую релевантность. И, наконец, выполним такой запрос select ts_rank_cd ( to_tsvector ( 'corpora.noun_verb' , 'Обама и Меркель встретились с Медведевым' ), plainto_tsquery ( 'corpora.noun_verb' , 'обама встретился' )) Оценка релевантности уменьшилась = 0.05. Потому что слово \"Меркель\" было включено в tsvector и разделило искомые сущности \"Обама\" и \"встретился\". Такое изменением оценки релевантности вполне соответствует смыслу - сначала нас интересуют события \"Обама встретился с ...\" и только потом \"Обама и ... встретились с ...\" Функция ts_rank_cd() имеет еше дополнительные инструменты для более тонкой настройки расчета релевантности. Рассмотрим такой пример. Опрелелены две исходные фразы \"Обама встретился с Медведевым\" \"Обама встретился с Медведевым и Меркель\" И задан поисковый запрос \"Обама встретился с Медведевым\". Понятно, что первая фраза будет более релевантной поисковому запросу, чем вторая. Но вычисление оценки релевантности для обоих фраз способом выше даст одинаковый результат - фразы будут отценены как эквивалентные. Вместе с тем, Функция ts_rank_cd() имеет дополнительный необязательный параметр, учитывающей при расчете релевантности особенности построения исхолной фразы, а точнее - особенности вектора tsvector, составленного конфигурацией из исходной фразы. Эти особенности учитываются следующим образом: расчетная оценка релевантности делится на коэффициент, в качестве которого может выступать длина tsvector, логарифм длины, количество уникальных лексеем в векторе и др. коэффициент влияет только степень назличия, подробнее можно прочитать в этой статье . Поэтому, выполнив такой запрос select ts_rank_cd ( to_tsvector ( 'corpora.noun_verb' , 'Обама встретился с Медведевым и Меркель' ), plainto_tsquery ( 'corpora.noun_verb' , 'обама встретился с Медведевым ' ), 2 ); мы получим различные оценки релевантности для наших исходных фраз. Что и требуется для решения поставленной задачи Итог После всех экспериментов с настройками, запрос поиска для сформулированной задачи будет выглядеть следующим образом. select sent , ts_rank_cd ( to_tsvector ( '{{schema_name}}.noun_verb' , sent ), plainto_tsquery ( '{{schema_name}}.noun_verb' , 'обама встретилcя' ), 2 ) as rank from {{ schema_name }} . news where to_tsvector ( '{{schema_name}}.noun_verb' , sent ) @@ plainto_tsquery ( '{{schema_name}}.noun_verb' , 'обама встретилcя' ) order by rank desc Послесловие За пределами данной статьи осталось рассмотрение случая назначения категорий для отдельных частей исходных текстов и некоторые полезные функции полнотекстрового поиска. Однакомиться с ними можно в этой публикации . Дополнение Если в какой-то момент изменится форимулировка прикладной задачи, например станет актуальным поиск не связки объект-событие, а нужно будет находить в тексте описательные характеристики объектов, достаточно будет создать новую конфигурацию по методике, приведенной в данной статье. Вопросы администрирования Как правило, ресурсы, выделяемые по базу ограничены. Поэтому полезно иметь несколько инструментов для их контроля и управления. Прочитать о них можно в этой статье Полезные ссылки Наборы текстов для исследования №1 (на различных языках) Наборы текстов для исследования №2 (на русском, английском и чешском) Корпус текстов из Википедии (в том числе и на русском)","tags":"Базы данных","title":"Postgresql: Полнотекстовый поиск (практический пример)"},{"url":"https://zlukfo.bitbucket.io/articles/python-profile","text":"Задача публикации - рассмотреть некоторые средства профилирования для скриптов python. В качестве примера будем анализировать такую функцию def regex ( c = None , r = True ): if not r : res = re . findall ( 'абр' , s ) return res if not c : res = re . findall ( r'а[бв]+р' , s ) return res res = c . findall ( s ) return res Общее время выполнения работы скрипта Общее время можно оценить минимум двумя способами - средствами системы и средствами python 1. Оценка времени средствами системы Создаем такой скрипт if __name__ == '__main__' : s = 'абракадабра' for i in xrange ( 10000 ): regex () и запускаем его time python test.py На выходе получаем real 0m0.045s user 0m0.032s sys 0m0.012s 1. Оценка времени средствами python Добавляем в скрипт небольшой класс import time class Timer ( object ): def __enter__ ( self ): self . start = time . time () return self def __exit__ ( self , * args ): self . end = time . time () self . secs = self . end - self . start if __name__ == '__main__' : with Timer () as t : for i in xrange ( 10000 ): regex () print \"=> time: %s s\" % t . secs и запускаем скрипт. На выходе = > time: 0.0494661331177 s Разбираемся, что мы получили в первом и втором случаях Но это, так сказать поверхностное тестирование - по результатам мы ничего не можем сказать о том сущесвует ли возможность сделать наш скрипт быстрее. Чтобы выяснить это - усланавливаем модуль профилировщика pip install line_profiler оборачиваем функцию regex() декоратором @profile и запускаем скрипт if __name__ == '__main__' : s = 'абракадабра' for i in xrange ( 10000 ): regex () kernprof -l -v test.py Теперь у нас есть более информативный результат общее время выполнения функции сколько раз была выполнена каждая инструкция функции, время выполнение инструкции при каждом вызове, общее время выполнения каждой инструкции, процент времени выполнения инструкции в общем времени выполнения функции Теперь рассмотрим, как эту информацию можно использовать применительно к анализируемой функции. Как видно, функция regex() может быть вызвана тремя способами regex()- поиск подстроки будет выполняться по регурярному выражению regex(r=False) - поиск будет выполняться по строковой константе (без регулярного выражения) regex(p)- поиск выполняется по заранее скомпилированному шаблону Оценим выполнение функции для каждого из вариантов. Поскольку regex() по сути состоит из одной функции re.findall(), будем также фиксировать и ее время работы (общее время работы функции / время выполнения re.findall()) 0.172847 / 110148 0.136507 / 95330 0.100585 / 42084 Уже на данном этапе анализа можно сделать некоторые выводы если искомая подстрока не содержит символы, идентифицирующие ее как регулярное выражение, то функция re.findall() выполняется быстрее (правда в данном случае для поиска подстроки она не нужна). Ктати, с увеличением количества обращений к функции разница в скорости выполнения междуд вариантами 1 и 2 будет сокращаться. По-видимому, это связано с тем, что функция findall (как и все остальные функции модуля re) сначала вызывает функцию compile, которая а) позволяет гораздо быстрее выполнять функции модуля и б) сохраняет 100 последних скомпилированных строк регулярного выражения, что позволяет избежать их повторной компиляции при неоднократном вызове функций модуля re, шаблон регулярного выражения полезно предварительно компилировать. В варианте 3 эта операция была вынесена за пределы функции regex, что позволило увеличить общую скорость ее работы на 70% Теперь подтвердим наши выводы. Для этого используем еще один инструмент профелирования cProfile. Выполним такой анализ скорости работы функции regex() для каждого из трех вариантов вызова python -m cProfile -s time -o test1.prof test.py Получим три файла профилирования. Анализировать их удобнее в графической оболочке kcachegrind. sudo apt-get install kcachegrind Кстати, для Windows есть порт программы https://sourceforge.net/projects/precompiledbin/ Для этого сначала сконвертируем их в нужный формат sudo pip install pyprof2calltree pyprof2calltree -i test1.prof -o test.kgrind а затем откроем в графической оболочке kcachegrind test.kgrind Наши выводы подтверждаются - только в третьем варианте вызова функции regex() функция _compile вызывается только один раз. Во всех остальных - по количеству обращений к функции findall(). И именно эти излишние обращения увеличивают общее время работы скрипта","tags":"Разработка","title":"Python: Профилирование скриптов. Анализ скорости выполнения"},{"url":"https://zlukfo.bitbucket.io/articles/generator-ispell","text":"Данная задача родилась из одной практической проблемы. В Сети можно легко найти ispell-словарь русского языка общей лексики, например здесь . Где используются такие словари, можно прочитать по этой ссылке . Но на тот момент было главное, что ispell-словари являются важной частью полнотекстового поиска в базах данных postgresql. Также важным было то, что в разрабатываемом проекте пользовательский поиск выполнялся c акцентом на географические названия и фамилии. Которых в общем словаре не так уж много. Поэтому возникла задача поиска (составления, генерации, ...) дополнительных словарей ispell необходимой тематики. В данной публикации приводится решение этой задачи с попыткой его максимального обобщения. После экспериментов был выбран следующий подход решения задачи Генерация словаря ispell выполняется на основе файла-списка необходимых слов. Получить такой список можно минимум двумя способами: а) составить вручную, б) выделить из словаря языкового корпуса, например, русского или британского Каждое слово словаря приводится в нормальную форму. Для этого используются морфологические анализаторы. Для русского языка можно использовать mystem от Yandex или библиотеку python pymorphy2 . Для английского языка можно использовать nltk Для нормальной формы слова подбирается набор правил образования словоформ на основе составленных правил для ispell-словарей. Скачать эти правила для различных языков можно по этой ссылке Данный подход реализован в скрипте на python, код приводится в конце публикации. Скрипт реализован для решения задачи со следующими исходными условиями: словарь генерируется для русского языка на вход скрипту может передаваться словарь Opencorpora или файл с произвольным списком слов Использование словаря Opencorpora имеет ряд преимуществ: на сегодня это самый объемный словарь для русского языка, доступный для свободного скачивания (содержит более 400 тыс. уникальных слов) каждая словоформа словаря характеризуется морфологическими признаками (граммеми). Это позволяет выделять для генерируемого словаря только слова нужного типа (например, географические названия или глаголы) Для приведения слова к нормальной форме в скрипте используется библиотека pymorphy2. Помимо скорости работы и высокого качества распознавания слов, ее преимущество еще и в том, что работа библиотеки основана на словаре Opencorpora и имеет тот же набор граммем. Вот полный код скрипта # -*- coding: utf-8 -*- import codecs import re def getFromOpencorpora ( filename , grammem ): ''' Function: getWFromOpencorpora Summary: извлекает из словаря Opencorpora нормальную форму слова (соответсвующую заданному набору граммем) и ее словоформы Examples: Attributes: @param (filename): путь к имени файла словаря Opencorpora @param (grammem): массив граммем (признаков) слова, которые необходимо извлекать из словаря. По сути является фильтром для извлечения слов нужного типа (глагол, одущивленное сущ. и т.д.) Returns: Возвращает итератор на генератор пары (список) (исходная форма слова, массив словоформ) ''' DELIM = '[,\\s]+' if type ( grammem ) == type ( '' ): grammem = [ grammem ] f = codecs . open ( filename , 'r' , 'utf-8' ) words_form = [] ps = re . compile ( DELIM ) for line in f : g = ps . split ( line . replace ( ' \\n ' , '' )) if len ( g ) > 1 : w = g . pop ( 0 ) if set ( grammem ) <= set ( g ) or words_form : words_form . append ( w ) if len ( line ) < 2 and words_form : n = words_form . pop ( 0 ) ff = set ([ i . lower () for i in words_form if n != i ]) yield [ n . lower (), ff ] words_form = [] f . close () def getFromListFile ( filename ): ''' Function: getFromListFile Summary: возвращает каждое слово из файла, хранящего пользовательский список слов для которого нужно сформировать правила Examples: Attributes: @param (filename): имя файла Returns: Возвращает итератор на генератор пары (список) (исходная форма слова, массив словоформ) ''' for f in codecs . open ( filename , 'r' , 'utf-8' ): yield [ f . replace ( ' \\n ' , '' ) . lower (), set ()] import pymorphy2 def getFromPymorphy ( word , grammem , morph ): ''' Function: getFromPymorphy Summary: для исходного слова и заданного набора граммем генерирует исходну форму и возможные словоформы Examples: Attributes: @param (word): исходное слово @param (grammem): массив нужных граммем для извлекаемого слова и словоформ @param (morph): объект морфологического анализатора morph = pymorphy2.MorphAnalyzer() Returns: возвращает список пар - (исходная форма, список словоформ), соответсвующий заданным грамменам ''' wf = [] if type ( grammem ) == type ( '' ): grammem = [ grammem ] # исходное слово может распознаться как относящееся к различным типам (по набору граммем) # например, 'туши' # на практике такого не должно быть - исходный набор граммем нужно задавать так, # чтобы однозначно определять категорию слова for j in morph . parse ( word ): f = set () for i in j . lexeme : if set ( grammem ) <= i . tag . grammemes : f . add ( i . word ) if ( j . normal_form , f ) not in wf : wf . append (( j . normal_form , f )) return wf def getAffixRules ( filename ): r = [] f = codecs . open ( filename , 'r' , 'utf-8' ) for i in f : ii = re . split ( r'\\s+' , i [: - 1 ]) if len ( ii ) >= 5 : if ii [ 2 ] == '0' : ii [ 2 ] = '' if ii [ 3 ] == '0' : ii [ 3 ] = '' r . append ( ii ) return r f . close () if __name__ == '__main__' : grammem = [ 'VERB' ] f_out_dict = codecs . open ( \" %s .dict\" % ( '_' . join ( grammem ) . lower (),), 'w' , 'utf-8' ) rc = [( re . compile ( j [ 4 ] + '$' ), re . compile ( j [ 2 ] + '$' ), j [ 3 ], j [ 1 ]) for j in getAffixRules ( 'ru.affix' )] morph = pymorphy2 . MorphAnalyzer () for word , wf in getFromOpencorpora ( \"dict.opcorpora.txt\" , grammem ): #for word, wf in getFromListFile(\"my_dict.txt\"): for normal_form , words_forms in getFromPymorphy ( word , grammem , morph ): r = set () for p in rc : if p [ 0 ] . findall ( normal_form ): if p [ 1 ] . sub ( p [ 2 ], normal_form ) in words_forms : r . add ( p [ 3 ]) if r : f_out_dict . write ( ' %s / %s \\n ' % ( normal_form , '' . join ( r ))) f_out_dict . close () Для работы со словарем Opencorpora используется функция getWFromOpencorpora, для работы с произвольным файлом-списком - getFromListFile. Для того, чтобы выбрать из Opencorpora только слова нужных типов, необходимо в переменной grammem задать список необходимых граммем. Обозначения граммем можно посмотреть на этой странице Практическую пользу от данного скрипта (и от этой публикации) можно оценить, ознакомившись с материалами этой статьи Англоязычные словари ispell Описанный подход также применяться для генерации словарей ispell для необходимых слов английского языка (и других тоже). Главное здесь - найти подходящий языковой корпус. Например, скрипт для генерации словаря ispell на основе Британского национального корпуса выглядит примерно так Полезные ссылки Все граммемы OpenCorpora Ресурс с русскими словарями (словарь синонимов, кладра НП россии (только им падеж), тезаурус) Алгоритм работы ispell-словарей","tags":"Анализ текста","title":"Генерация словарей ispell"},{"url":"https://zlukfo.bitbucket.io/articles/postgresql-fts","text":"Кто не знает, что такое полнотекстовый поиск, как он организован в postgresql, рекомендую прочитать тут . В этой статье рассмотрены ключевые моменты настройки полнотектового поиска. Наиболее важными элементами настройки полнотектового поиска (fts) - являются словари и конфигурации. Но начшем мы с другого элемента настройки fts. Парсер (анализатор) Задача парсера - разбить фрагмент текста на слова для последующего анализа каждого слова словарями, включенными в конфигурацию полнотектового поиска. Рассмотрение парсера fts выходит за замки данной статьи, и вот почему. В postgresql из коробки реализован единственный парсер default, который естественным (для большинства задач) способом разделяет фрагмент текста на слова - каждое слово отделяется от остальных пробелами, знаками препинания, переносом строки и пр. Кроме того, парсер default каждое слово присваивает к одной из 23 предопределенных групп. Это позволяет в конфигурации fts настраивать словари для анализа не всех, а только необходимых групп слов. Посмотреть, какие группы слов различает парсер default можно так select * from ts_token_type ( 'default' ) А посмотреть пример как парсер default разделяет фрагмент текста на слова и как классифицируюет слова по группам можно так select * from ts_parse ( 'default' , 'email:osp2003@inbox.ru;пользователь User;http://goszakupki.tk;rating:123.12' ) as bar INNER JOIN ( select * from ts_token_type ( 'default' )) as foo ON bar . tokid = foo . tokid ; Парсера defaul достаточно для решения многих прикладных задач, поэтому при настройке полнотектового поиска вопрос выбора парсера не актуален. Но кому интересно - может написать собственный парсер fts и интегрировать его в postgresql. Парсеры пишутся на C. Пример написания собственного парсера можно найти в этой статье Словари Словарь - это программа, которая выполняет две задачи 1) нормализацию слов, 2) исключение стоп-слов. Стоп-слова - это такие слова, которые не должны учитываться в результатах полнотектового поиска. Потому что не несут никакой информационной нагрузки. К стоп-словам прежде всего относятся такие части речи, как предлоги, союзы и пр. Но в итоге администратор базы данных сам определяет список стоп слов для конкретного проекта. Все словари (программы) на выходе работают по одинаковому алгоритму - если слово является стопом - возвращается пустой массив, если нет - возвращается преобразованное значение слова (преобразование выполняется каждым словарем по разному, в зависимости от его назначения) \"Из коробки\" в postgresql включает следующие типы (шаблоны) словарей: simple - возвращает слово, приведенное к нижнему регистру. Дополнителный параметр Accept может переопределить работу алгоритма, вовращая NULL словарь синонимов - Возвращает синоним найденного слова. Не поддерживает словосочетания тезаурус - расширение словаря синонимов (поддерживает словосочетания). словарь ispell - выполняет лингвистическую нормализацию слов. обращается к файлам словарям, составленным на различных языках. скачать словари можно по этому адресу словарь Snowball - выполняет обрезание слов оставляя его значимую часть. Еще \"из коробки\" в postgresql есть словари стоп-слов, том числе и для русского языка - russian.stop. Словарь небольшой, содержит предлоги, союзы и пр. Все словари хранятся в /usr/share/postgresql/9.6/tsearch_data/ Созданием пользовательского словаря в postgresql управляет команда CREATE TEXT SEARCH DICTIONARY dict_name ( TEMPLATE = ispell , ... другие параметры ... ); т.е., обязательный параметр - указание шаблона для создаваемого словаря. Изначально в postgresql встроено несколько типов шаблонов, если их недостаточно можно написать собственный. Шаблоны пишутся на языке C, рассмотрение этого процесса выходит за рамки статьи. В качестве необязательного параметра при создании пользовательского словаря можно указать список стоп-слов. Здесь нужно учитывать один момент. В каждом наборе текстов определенной тематике существуют частоупотребимые слова, включенные почти в каждый текст. Очевидно, поиск по таким словам бесполезен - в результате запроса мы получим выборку из всех текстов, где встречаются эти слова. Поэтому, частоупотребимые слова логично отнести к списку стоп-слов. Как найти такие слова? Выполним запрос SELECT * FROM ts_stat ( 'select to_tsvector(''{{fts_configuration}}'', {{_field_name_}}) as word from {{_schema_name_}}.{{_table_name_}}' ) ORDER BY nentry DESC , ndoc DESC , word LIMIT 100 ; Функция to_tsvector преобразует исходный текст в формат tsvector. Первый параметр (не обязательный) - это имя конфигурации, которая будет проводить преобразование. Если параметр не указывается, используется конфигурация по умолчанию. конфигурация 'russian' как правило доступна \"из коробки\". Посмотреть, какие словари доступны можно так select d_t . dictname , d_t . dictinitoption , d_t . tmplname , n . nspname , n . nspacl from ( select d . dictname , d . dictinitoption , d . dictnamespace , t . tmplname from pg_ts_dict as d join pg_ts_template as t on ( d . dicttemplate = t . oid )) as d_t join pg_namespace as n on ( d_t . dictnamespace = n . oid ) К словарям, созданным на основе шаблонов simple, ispell, snowball можно подключить собственный список стоп-слов. Это важно потому, что шаблоны по разному работают с этим списком. ispell - сначала нормализует слова, а затем просматривает список стоп-слов snowball - сначала просматривает список, а затем работает simple - ...!!! Это важно учитывать когда мы будем создавать конфигурацию fts. Проверить, как словарь, созданный на основе конкретного шаблона обработает слово можно так select ts_lexize ( 'russian_stem' , 'Новостной' ); Словари и конфигурации создаются для каждой схемы базы данных. Кроме того, можно использовать встроенные словари и конфигурации, если алгоритм их работы подходит для конкретной практической задачи. Конфигурация Конфигурация - это способ соединения созданных словарей в цепочку. Еще конфигурация определяет используемый тип парсера, но в рамках данной статьи (и многих приктических проектов) используется парсер по умолчанию, поэтому не будем акцентировать на нем внимание. Создание конфигурации Создать собственную конфигурацию можно двумя способами Скопировав в нее настройки одной из встроенных конфигураций CREATE TEXT SEARCH CONFIGURATION {{ confname }} ( COPY = {{ existconf }} ) В этом случае созданная конфигурация наследует от встроенной все настройки (которые можно потом менять) парсер, разбивающий текст на слова и классифицирующий их по типам набор словарей, каждый из которых обрабатывает один или несколько типов слов порядок, в котором словари будут обрабатывать слова Создать \"чистую\" конфигурацию CREATE TEXT SEARCH CONFIGURATION {{ confname }} ( PARSER = {{ parsername }} ) В этом случае мы только задаем парсер, разбивающий текст на слова. Настройка цепочко словарей для каждого типа слов, распознанных парсером, выполняется самостоятельно. Кстати, за конфигурацию fts по умолчанию отвечает параметр postgresql default_text_search_config. show default_text_search_config ; set default_text_search_config = simple ; Просмотр конфигураций Первый вариант создания конфигурации более простой и быстрый - основные настройки будут скопированы из встроенной конфигурации и нужно лишь добавить собственные (иначе, зачем создавать новую конфигурацию когда можно воспользоваться встроенной?) Для того, чтобы посмотреть информацию о встроенных конфигурациях можно выполнить запрос. select res . cfgname , tt . alias , res . mapseqno , res . dictname , tt . description from ( select cm_d . mapseqno , c . cfgname , cm_d . maptokentype , cm_d . dictname from pg_ts_config as c join ( select * from pg_ts_config_map as cm join pg_ts_dict as d on ( cm . mapdict = d . oid )) as cm_d on ( c . oid = cm_d . mapcfg ) ) as res join ( select * from ts_token_type ( 'default' )) as tt on ( res . maptokentype = tt . tokid ) order by res . cfgname , res . maptokentype , res . mapseqno В результате получим cfgname - имя конфигурации alias - типы слов для которых в конфигурации задана обработка dictname - словари, обрабатывающие слова данного типа mapseqno - порядок обработки словарями данного типа слов description - описание типа слов Если создать конфигурацию на основе встроенной, то выполнив запрос выше увидим, что встроенная и пользовательская конфигурции одинаковы и можно вносить в пользовательскую необходимые изменения Если же мы создаем \"чистую\" конфигурацию (вариант 2), в результатах запроса выше ее не будет потому что мы пока еще не назначили словари для обработки типов слов Настройка конфигурации Для настройки созданной конфигурации используется команда ALTER TEXT SEARCH CONFIGURATION Она имеет несколько контекстов вызовов, прочитать прокоторые можно в документации Пример создания конфигурации Чтобы лучше понимать, как работает конфигурация полнотекстового поиска, будем создавать \"чистую\" конфигурацию. Допустим, в нашей базе уже имеются два пользовательских рускоязычных словаря - словарь существительных noun и словарь глаголов infn, созданных на базе собственных словарей ispell. Как генерировать собственные словари ispell можно прочитать в этой публикации. Создаем конфигурацию CREATE TEXT SEARCH CONFIGURATION conf_test ( PARSER = default ) Планируем работу конфигурации В исходном тексте конфигурация должна различать только русскоязычные слова, являющиеся глаголом или существительным. Все остальные слова должны игнорироваться. Приоритета распознавания между глаголом и существительным нет. Определяемся с нужными для конфигурации типами слов (на примере произвольной тестовой строки). select * from ts_parse ( 'default' , 'email:osp2003@inbox.ru;пользователь User;http://goszakupki.tk;rating:123.12' ) as bar INNER JOIN ( select * from ts_token_type ( 'default' )) as foo ON bar . tokid = foo . tokid ; По результатам видим, что русскоязычные слова относятся к типу word. Создаем конфигурацию ALTER TEXT SEARCH CONFIGURATION corpora . conf_test ADD MAPPING FOR word WITH noun , infn Тестируем select * from to_tsvector ( 'conf_test' , 'Встретились с ПУТИНЫМ вчера ранним вечером в 19' ) Что-то не так - конфигурация распознала лексему 'сереть' которой быть не должно. Причина в побочном эффекте - союз 'с' не вколючен в список стоп-слов словарей noun и infn. Поэтому было применено одно из заданных в ispell правил - \"если из слова исключить -ереть и ничего не добавлять - получится сереть. Поэтому союз c и был преобразован в слово сереть.Выход - пересоздать словари существительных и глаголов - добавив в них список стоп-слов Релевантность поиска напрямую зависит от созданной цепочки словарей. Поэтому важно понимать как цепочка работает. Каждый словарь в цепочке работает по единому принципу возвращает массив лексем, если входное слово известно словарю (заметьте, один фрагмент может породить несколько лексем) возвращает одну лексему с установленным флагом TSL_FILTER для замены исходного фрагмента новым, чтобы следующие словари работали с новым вариантом (словарь, который делает это, называется фильтрующим словарём) возвращает пустой массив, если словарь воспринимает этот фрагмент, но считает его стоп-словом NULL, если словарь не воспринимает полученный фрагмент Релевантность поиска Использование индексов Практика создания полнотекстового поиска разобрана на примере в следуюшей статье. Рекомендуется к прочтению http://pgday.ru/files/papers/101/master-pgday16.pdf Как работает snowball <правилам","tags":"Базы данных","title":"Postgresql: Полнотекстовый поиск (теория)"},{"url":"https://zlukfo.bitbucket.io/articles/postgresql-patrition","text":"Что такое секционирование, зачем оно нужно и как насторить, достаточно подробно описано здесь В этой статье приводится один способ упросить процесс настройки секционирования. Собственно способ сводится к запуску python-скрипта, который подключается к существующей БД postgresql (в скрипте задаются параметры подключения) загружает шаблон sql-скрипта и подставляет в него необходимые параметры (см. ниже). Параметры задаются в python-скрипте. выполняет sql-скрипт. Условия и ограничения Описываемый здесь способ секционирования подходит для записей, содержащих поле с датой. По значению этого поля запись сохраняется в соответствующую таблицу. В качестве временного интервала для каждой таблицы принят один месяц. Вся работа выполняется командами sql. Python-скрипт является всего лишь оболочкой для удобного ввода исходных значений и запуска sql-скрипта. В качестве исходных данных в python-скрипте нужно указать '_schema_name_' - имя создаваемой схемы, '_master_table_' - имя главной таблицы, '_record_date_' - имя поля даты, по которому будет выполняться секционирование Sql-скрипт выполняет следующие операции создает в БД схему с именем '_schema_name_' создает новую таблицу с именем '_master_table_'. !!! Это важный момент - необходимые поля для таблицы нужно указать самостоятельно прямо в шаблоне. Для поля с датой, по которому будет выполняться секционирование в качестве имени указать {{_record_date_}} создает триггерную функцию и триггер. Как это работает Перед каждым добавлением новой записи в таблицу '_master_table_' задействуется триггер, который: а) сопоставляет значение даты для поля {{_record_date_}} с именем дочерней таблицы б) проверяет, существует ли дочерняя таблица с таким именем. Если не существует - создает ее в) добавляет новую запись в соотвествующую дочернюю таблицу Исходный код можно скачать здесь.","tags":"Базы данных","title":"Postgresql: один способ организовать секционирование таблиц"},{"url":"https://zlukfo.bitbucket.io/articles/postgresql-rules","text":"Выделение прав на доступ к объектам базы данных будем рассматривать на практическом примере из этой статьи. Задача и требования к правам Имеется база данных parser , включающая следующие схемы config - схема хранящая конфигурацию logs - схема хранения логов [project_data1, project_data2,...] - одна или более схем с данными различных проектов Нужно создать пользователя external_user , наделенного следующими правами Пользователь может иметь доступ только к базе данных parser. Доступ к информации других баз данных в данном кластере пользователю запрещен. Пользователь имеет доступ только к схемам config и logs. Доступ к схемам с данными проектов пользователю запрещен. В схеме config определена функция getQuery() и материализованное представление _. Пользователь имеет доступ только к этим объектам. Доступ к другим объектам схемы (таблицам, представлениям, последовательностям и т.д.) пользователю запрещен. В схеме logs от имени пользователя может выполняться создание дочерних таблиц логов и добавление в них записей. Остальные операции с таблицами запрещены. Доступ к публичным схемам information_schema и pg_catalog максимально ограничен. Имеется доступ только к объектам, доступным для выполнения функции config.getQuery() Реализация правил Добавляем пользователя и устанавливаем пароль для него. create role external_user with LOGIN ; ALTER USER external_user WITH PASSWORD 'qweasd' ; Начиная с версии 8.1 в postgresql группы пользователей и пользователи объединены в одну концепцию ролей. Это значит, что команда create role может создавать как групповые роли (группы пользователей), так и роли входа (пользователей). Выражение with LOGIN указывает на то, что мы создаем роль входа (пользователя). Второй командой мы задаем пароль для пользователя. Примечание. Для групповой роль также может быть назначен пароль. На данном шаге следует запомнить два момента. пользователь создается на уровне кластера баз данных, а не отдельной базы каждый созданный пользователь автоматически наделяется правами PUBLIC - эта групповая роль, которая существует в любом кластере. Для таблиц, столбцов, схем и табличных пространств роль PUBLIC не имеет никаких прав. Для баз данных роль имеет права CONNECT (подключение) и CREATE_TEMP_TABLE (создание временных таблиц). Для функций - EXECUTE (выполнение), для языков - USAGE (использование). Из этого следует важный вывод - созданные пользователь может подключиться к любой базе данных кластера. Правда при этом не иметь доступа к таблицам ее схем, поскольку у него нет доступа к самим схемам. Важность этого вывода в следующем. В любой базе данных postgresql существует информационная схема с именем information_schema , которая содержит набор представлений, содержащих информацию об объектах текущей базы данных. И еще есть схема pg_catalog , также содержащая объекты (таблицы, функции, представления) о текущей базе данных. Поэтому созданный пользователь не имея еще фактически никаких прав, уже может узнать много полезной информации о базе данных, к которой он подключился и о кластере баз. Например -- посмотреть пользователей зарегистрированных в кластере баз данных и их права select * from pg_catalog . pg_user ; -- посмотреть список баз данных кластера select datname from pg_catalog . pg_stat_database ; -- посмотреть существующие схемы и таблицы кластера баз данных select * from pg_catalog . pg_tables ; и так далее. Данная информация потенциально может быть опасной. Поэтому наш следующий шаг - закрываем для созданного пользователя доступ к объектам схем Отступление Здесь важно понять механизм исключения прав. Создаваемый пользователь наследует права роли PUBLIC. Поэтому, чтобы лишить его какого-либо права нужно лишить этого права PUBLIC. Или создать другую групповую роль с нужными правами и затем предоставлять в ней членство создаваемым пользователям (в данной публикации этот вопрос не рассматривается) Закрываем созданному пользователю доступ к схемам information_schema, pg_catalog. На данном этапе существуют определенные тонкости со схемой pg_catalog. Во-первых, схема хранит много служебной информации, необходимой для работы пользовательских функций. Так, если закрыть для пользователя external_user доструп к pg_catalog, то вызов даже такой простой функции как getQuery() (описанной в этой статье) будет генерировать ошибку. В частности потому, что в каталоге pg_catalog хранится информация о всеъ типах данных postgresql. А во-вторых существует особенность доступа к объектам схемы pg_catalog. А именно при разборе sql-запроса postgresql автоматически добавляет схему pg_catalog в пути поиска. Поэтому, мы можем закрыть доступ к схеме pg_catalog и запрос типа SELECT * FROM pg_catalog . pg_roles ; подскажет нам об этом. Но автоматическое добавление pg_catalog в путь поиска позволяет нам выполнить запрос SELECT * FROM pg_roles ; который вернет нам зарегистрированных в кластере пользователей. Поэтому, если мы хотим максимально ограничить доступ пользователя к схеме pg_catalog, воспользуемся следующими командами Забираем у групповой роли PUBLIC права на доступ к таблицам, представлениям и выполнению функций REVOKE ALL PRIVILEGES ON ALL TABLES IN SCHEMA pg_catalog FROM PUBLIC ; REVOKE ALL PRIVILEGES ON ALL FUNCTIONS IN SCHEMA pg_catalog from public ; Предоставляем права пользователю external_user только для нужных объектов (в нашем примере - таблицы pg_class, pg_type) grant select on pg_class , pg_type to external_user Закрываем доступ к подключению других баз данных (на примере бд other_db) SET SESSION AUTHORIZATION postgres ; REVOKE connect ON database other_db FROM PUBLIC ; Добавляем доступ к объектам схемам config и logs GRANT USAGE ON SCHEMA config TO external_user GRANT USAGE ON SCHEMA logs TO external_user При этом, после после получения доступа к схеме, наследуя права PUBLIC стали доступны следующие операции запуск всех функций, хранящихся в схеме, создание временных таблиц, возможность использования языков Ограничиваем доступные для выполнения функции. Если нужно запретить доступ к отдельным функциям, можно сделать так (на примере функции a()) REVOKE execute ON function config . a () FROM public ; Внимание! Если функция предусматривает входные параметры, при ее указании в скобках нужно перечислить типы этих параметров. Если же функций много, а разрешить выполнение нужно только некоторым, делаем так (на примере функции a()) -- запрещаем для роли public выполнение всех функций REVOKE ALL PRIVILEGES ON ALL FUNCTIONS IN SCHEMA config from public ; --даем разрешение на выполнение только тем пользователям, которым нужно GRANT execute ON function config . a () TO external_user ; Напоследок проверяем какими правами обладает наш пользовател external_user в полкюченной базе. Для этого выполняем запрос. select * from ( SELECT use . usename as subject , nsp . nspname as schem , c . relname as obj , c . relkind as type , use2 . usename as owner , c . relacl , ( use2 . usename != use . usename and c . relacl :: text !~ ( '({|,)' || use . usename || '=' )) as public FROM pg_user use cross join pg_class c left join pg_namespace nsp on ( c . relnamespace = nsp . oid ) left join pg_user use2 on ( c . relowner = use2 . usesysid ) WHERE c . relowner = use . usesysid or c . relacl :: text ~ ( '({|,)(|' || use . usename || ')=' ) ORDER BY subject , schem , obj ) as res where subject = 'external_user' and public = false ; Запрос работает в пределах текущей подключенной базы данных. И не информативен для роли с правами суперпользователя Запрос содержит следующие поля subject - имя пользователя schem - схема хранения объекта obj - название конечного объекта на который у пользователя есть права type - метка типа объекта owner - владелец объекта relacl - права пользователя на объект public - метка принадлежности к публичной схеме information_schema или pg_catalog Метка типа объекта r = обычная таблица, i = индекс (index), S = последовательность (sequence), v = представление (view), m = материализованное представление(materialized view), c = составной тип (composite), t = таблица TOAST, f = сторонняя таблица (foreign) Права пользоватя на объект Записи, выводимые запросом интерпретируются так: имя_роли=xxxx -- права, назначенные роли =xxxx -- права, назначенные PUBLIC r - SELECT (\"read\", чтение) w - UPDATE (\"write\", запись) a - INSERT (\"append\", добавление) d - DELETE D - TRUNCATE x - REFERENCES t - TRIGGER X - EXECUTE U - USAGE C - CREATE c - CONNECT T - TEMPORARY arwdDxt - ALL PRIVILEGES (все права для таблиц; для других объектов другие) * - право передачи заданного права /yyyy -- роль, назначившая это право Запрос возвращает только те конечные объекты (таблицы, функции,... но не базы данных и схемы) у которых пользователю заданы какие-либо права. При этом, из результатов исключаются объекты, принадлежащие общим схемам information_schema и pg_catalog. Если мы хотим включить и их - нужно закомментировать условие public=false Как было сказано ранее, запрос возвращает права доступа пользователя к объектам текущей подключенной бд. Чтобы узнать его права для объектов другой бд кластера postgresql, сначала к этой бд нужно подключиться. Для этого будем использовать расширение postgresql dblink. sudo apt-get install postgresql-contrib CREATE EXTENSION dblink ; (помним, что расширение создается для текущей бд) Теперь запрос на получение прав пользователя будет выглядеть так SELECT * FROM dblink ( 'dbname={{database_name}}' , ' select * from ( SELECT use.usename as subject, nsp.nspname as schem, c.relname as obj, c.relkind as type, use2.usename as owner, c.relacl, (use2.usename != use.usename and c.relacl::text !~ (''({|,)'' || use.usename || ''='')) as public FROM pg_user use cross join pg_class c left join pg_namespace nsp on (c.relnamespace = nsp.oid) left join pg_user use2 on (c.relowner = use2.usesysid) WHERE c.relowner = use.usesysid or c.relacl::text ~ (''({|,)(|'' || use.usename || '')='') ORDER BY subject, schem, obj ) as res where subject = ''{{user_name}}''; ' ) AS r ( subject name , schem name , obj name , type char , owner name , relacl aclitem [], public boolean ) запрос должен выполняться от имени суперпользователя Узнать имена всех баз данных текущего класера можно так select datname from pg_database","tags":"Базы данных","title":"Postgresql: Выделение прав"},{"url":"https://zlukfo.bitbucket.io/articles/postgresql-map","text":"Контурые карты часто применяются в задачах визуализации, когда нужно отобразить некоторые статистические данные на геопространственных координатах. Для хранения контурных карт наиболее популярными форматами являются форматы shape, geojson и его модификация topojson, позволяющая хранить данные более компактно. Хранить эти форматы можно в базе данных postgtesql, при этом удобно использовать расширение postgis, предоставляющее различные вычислительные функции с географичесими координатами. Для отображения контурных карт в web-приложении используются различные библиотеки, например, d3js. Карты стран мира в формате shape Карты очертаний городов России по регионам Каждый архив содержит 2 типа карт. pop-built-up-a-rostov-gislab более полная, включает следующие полезные поля название на латитице и кирилице тип объекта (хутор, город,...) район код по КЛАДР еще один код?? напоминающий КЛАДР почтовый индекс географические координаты Карты СНГ, России и регионов (подробные с точностью до зданий) Каждый архив содержит несколько типов карт административные границы boundary-polygon.shp населенные пункты (в виде точек, не полигонов) settlement-point.shp здания building-polygon.shp автодороги highway-line.shp железные дороги railway-line.shp может еще копу пригодятся землепользование landuse-polygon.shp растительность vegetation-polygon.shp точки интереса poi-polygon.shp Карты городов (shape, geojson) Загрузка карт в базу данных Установливаем Postgis sudo apt-get install postgresql-9.6-postgis-2.3 create extension postgis ; Загружаем карты Существует удобная утилита shp2pgsql, позволяющая конвертировать карты в shape-формате в базу данных. Утилита устанавливается с пакетом postgis. Вот пример экспорта shape-файла в БД shp2pgsql _shapefile_ _schema_._tablename_ > _sqlfilename_ psql -h localhost -d geo -U postgres -f _sqlfilename_ Если напрямую в БД, то можно так shp2pgsql pop-built-up-a-rostov-gislab rostov1 | psql -h localhost -d geo -U postgres Если выдает ошибку, связанную с кодировкой при именах, написанных кирилицей, то попробовать добавить ключ -S -W \"utf-8\" Описание ключей команды (на русском) можно посмотреть здесь Для любителей кодить и более гибкой загрузки shape-файлов в БД существует удобная python-библиотека pyshp . Вот пример рабочего кода выполняющий ту же задачу, что и утилита shp2pgsql - экспорт shape-файла в таблицу БД # -*- coding: utf-8 -*- import shapefile import psycopg2 from psycopg2.extensions import QuotedString import json filename = \"ne_10m_admin_0_countries\" dbname = 'geo' myshp = open ( filename + \".shp\" , \"rb\" ) mydbf = open ( filename + \".dbf\" , \"rb\" ) myshx = open ( filename + \".shx\" , \"rb\" ) reader = shapefile . Reader ( shp = myshp , dbf = mydbf , shx = myshx ) fields = reader . fields [ 1 :] # подключаемся к БД conn = psycopg2 . connect ( \"dbname=' %s ' user='postgres' password='qweasd'\" % ( dbname ,)) conn . autocommit = True cur = conn . cursor () # собираем имена полей и их типы для создания таблицы ff = 'gid serial NOT NULL, ' for f in fields : typ = 'smallint' if f [ 1 ] == 'C' : typ = 'character varying( %d )' % ( f [ 2 ],) if f [ 1 ] == 'N' and f [ 3 ]: typ = 'double precision' ff += '\" %s \" %s , ' % ( f [ 0 ], typ ) # создаем таблицу в БД sql = \"CREATE TABLE IF NOT EXISTS %s ( %s )\" % ( filename , ff [: - 2 ]) cur . execute ( sql ) sql = \"SELECT AddGeometryColumn('public', ' %s ','geom','0','MULTIPOLYGON',2);\" % ( filename ,) cur . execute ( sql ) # заполняем таблицу fields = [ field [ 0 ] for field in fields ] fields . append ( 'geom' ) fields = str ( tuple ( fields )) . replace ( \"'\" , '\"' ) for sr in reader . shapeRecords (): data = ',' . join ([ QuotedString ( str ( i )) . getquoted () for i in sr . record ]) geo = sr . shape . __geo_interface__ if geo [ \"type\" ] == 'Polygon' : geo [ \"type\" ] = 'MultiPolygon' geo [ \"coordinates\" ] = [ geo [ 'coordinates' ]] sql = 'INSERT INTO \" %(table)s \" %(fields)s VALUES ( %(val)s , ST_GeomFromGeoJSON( %(geom)s ));' % { 'table' : filename , 'fields' : fields , 'val' : data , 'geom' : QuotedString ( json . dumps ( geo )) . getquoted ()} cur . execute ( sql ) Получить контурную карту из базы Наша карта сохраняется в таблице в специальном сжатом формате postgis. Для ее отрисовки в браузере карту нужно прежде всего преобразовать в формат geojson (о спецификации формата можно прочитать по ссылке вконце статьи). Преобразование выполняется с помощью команды ST_AsGeoJSON , например, так: select ST_AsGeoJSON ( geom ) from ne_10m_admin_0_countries where \"SOVEREIGNT\" = 'Russia' Если мы ходим объединить полигоны (все или по некоторому признаку), делаем примерно так SELECT ST_AsGeoJSON ( ST_UNION ( ARRAY ( select geom from rostov1 ))); Так можно объединять по различным признакам, например - показать населенные пункты, находящиеся на одной широте, выделить только города - миллионники и т.д. При необходимости, если выходной объем получается очень большой для передачи клиенту, геометрию карты можно упрощать с помошью команды ST_Simplify Проверить отрисовку в браузере Посмотреть корректность отображения контурной карты можно так. С помошью команды выше выгрузить в формате geojson интересующую нас карту в файл. Файл загрузить на один из сервисов http://geojson.io/#map=2/40.4/45.2 http://mapshaper.org/ Кстати, эти онлайн сервисы позволяют сконвертировать формат geojson в topojson - более компактный и адаптированный для работы с библиотекой d3js на стороне клиента. Полезные ссылки https://dikmax.name/post/map-tutorial/ Спецификация формата GeoJSON Примеры использования shp2pgsql и pgsql2shp http://geojson.io/#map=2/20.0/-0.2","tags":"Разработка","title":"Postgresql: хранение и извлечение контурных карт"},{"url":"https://zlukfo.bitbucket.io/articles/vds-postgresql-server","text":"Итак, у нас есть чистый VDS. Задача - создать на его базе сервер базы данных. В качестве БД используем postgresql 1. Настройка VDS Провайдеры VDS на чистом контейнере как правило предоставляют доступ по ssh под пользователем root. Изменим эти пастройки по умолчанию ssh root@__host__ apt-get update apt-get install language-pack-ru-base apt-get install mc useradd __adminName__ mkdir /home/__adminName__ chown __adminName__:__adminName__ /home/__adminName__ passwd __adminName__ apt-get install sudo visudo меняем \"root ALL=(ALL:ALL)\" ALL на \"adminName ALL=(root:root) ALL\" Команды для работы в редакторе: o - Создать текст с начала новой строки, расположенной под текущей строкой), ESC - закончить редактирование, :w - созранить изменения :q - выйти из редактора (все команды редактора vi здесь ) Проверяем, можем ли мы подключиться через __adminName__ ssh __adminName__@__host__ Закрываем пользователя для root Заходим под __adminName__, выполняем sudo passwd -l root Правим /etc/ssh/sshd_config: \"PermitRootLogin no\" Перезапускаем sshd. sudo service ssh restart Меняем командную оболочку по умолчанию sudo chsh __adminName__ В диалоге указываем путь к нужной оболочке. Доступные оболочки можно посмотреть в файле /etc/shells Установка последней версии postgresql sudo nano /etc/apt/sources.list.d/postgresql.list Добавляем строку \"deb http://apt.postgresql.org/pub/repos/apt/ xenial-pgdg main\" Выполняем команды wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - sudo apt-get update sudo apt-get install postgresql-9.6 Настройка postgresql sudo passwd postgres su postgres mkdir /var/lib/postgresql/data sudo nano /etc/environment Если на данном этапе будет выдавать такую ошибку \"в postgres отсутствует в файле sudoers\" , сделать следующее su __adminName__ sudo visudo postgres ALL =( ALL ) ALL su postgres sudo nano /etc/environment Внутри файла дополняем значение переменной PATH, дописав в него через двоеточие путь \"/usr/lib/postgresql/9.6/bin\". И, затем, в конце этого файла дописываем следующую строчку '\"PGDATA=\"/var/lib/postgresql/data\"' Завершаем сеанс (exit) и заходим заново su postgres initdb Редактируем файл sudo nano '/etc/postgresql/9.6/main/postgresql.conf'. В нем находим строчки (значения переменных не обязательно именно такие) data_directory = '/var/lib/postgresql/9.1/main' hba_file = '/etc/postgresql/9.1/main/pg_hba.conf' ident_file = '/etc/postgresql/9.1/main/pg_ident.conf' external_pid_file = '/var/run/postgresql/9.1-main.pid' и комментируем их, поставив вначале каждой строки знак \"#\". Теперь после перезагрузки системы PostgreSQL не будет автоматически стартовать. Для автозапуска PostgreSQL в Ubuntu нужно открыть файл sudo nano '/etc/rc.local' и в нем вписать строчку \"su -c 'pg_ctl start' postgres\" (до команды exit в файле). Перезапускаем VDS sudo reboot Удаленный доступ к бд По умолчанию postgres настроен для доступа только с локальной машины. Поэтому прежде всего огранизуем доступ к БД через ssh туннель ssh -L 63333:localhost:5432 zlukfo@188.120.236.2 Терминал не закзывать! Теперь мы можем подключиться к бд. Открываем новый терминал и подключаемся psql -h localhost -p 63333 -U postgres Для поклонников Sublime Text Можно настроить выполнение sql запросов из редактора. В нем же просматривать ответы. Для этого заходим Tools - > Build System -> New Build system . И пишем примерно такой код { \"cmd\" : [ \"/usr/bin/psql\" , \"-U\" , \"postgres\" , \"-h\" , \"localhost\" , \"-p\" , \"63333\" , \"-o\" , \"/home/zlukfo/result_sql.txt\" , \"-a\" , \"-E\" , \"-f\" , \"$file\" ] } и сохранаяем в предложенный по умолчанию каталог Создаем файл .sql и пишем туда один или несколько sql запросов. Сохраняем файл. Нажимаем ctl+shift+b . Внимание!!!. Конфигурацию выше нужно создавать для каждой БД (параметр -d psql). А перед тем как отправлять запрос (ctl+shift+b) - выбирать соотвествующую конфигурацию в меню Tools - > Build System Результат запроса сохранится в файл \"/home/zlukfo/result_sql.txt\", В Side Bar можно создать быструю ссылку на этот файл. Если при отурытии файла будут автоматически переноситься строки - добавить Preferenses -> settings-user \"word_wrap\": \"false\" Если очень нужно, можно открыть доступ с улаленных хостов /var/lib/postgresql/data/postgresql.cong listen_addresses = '*' pg_hba.conf host all postgres 0.0.0.0/0 md5 Перегружаемся su postgres pg_ctl restart Отправка файлов на сервер по ssh Очень частая задача, тем более, что она может быть решена без установки дополнительных программ scp /directory/some_file user@server_domain:/remote/directory","tags":"Администрирование","title":"VDS: создаем сервер базы данных на основе postgresql"},{"url":"https://zlukfo.bitbucket.io/articles/python-send-mail","text":"Здесь приведен небольшой скрипт на python который решает следующую задачу - рассылка писем на адреса электронной почты по списку . Условия работы скрипта: электронный адрес источника рассылки на Yandex.Почта скрипт не предусматривает отправку писем с вложениями Возможности работы скрипта: текст письма может быть составлен в формате html корректность отправки проведена на почтовых серверах Mail, Google, Yandex from smtplib import SMTP_SSL from email.MIMEText import MIMEText def sendMail ( client_mail , message , header ): my_donor_mail = \"...\" # адрес электронной почты источника рассылки msg = MIMEText ( message , \"html\" , \"utf-8\" ) msg [ 'Subject' ] = header msg [ 'From' ] = my_donor_mail msg [ 'To' ] = client_mail smtp = SMTP_SSL () smtp . connect ( 'smtp.yandex.ru' ) smtp . login ( my_donor_mail , '...' ) # пароль почты smtp . sendmail ( my_donor_mail , client_mail , msg . as_string ()) smtp . quit () mail_list = '...' # файл со списком адресов рассылки: один адрес - одна строка mail_body = '...' # файл с текстом письма header = '...' # заголовок письма f = open ( mail_body , 'r' ) message = f . read () f . close () for mail in open ( mail_list , 'r' ): try : sendMail ( mail , message , header ) print 'Send to: ' + mail [: - 1 ] except : print 'NOT send to: ' + mail [: - 1 ] f . close ()","tags":"Полезные утилиты","title":"Python: рассылка писем по электронной почте"},{"url":"https://zlukfo.bitbucket.io/articles/apache-upload-wsgi","text":"Загрузка файлов на http сервер выполняется методом POST. Большие файлы передаваются несколькими запросами. В качестве url POST-запроса указывается скрипт-обработчик, в нашем случае скрипт wsgi. Обработчик должен брать на себя полностью управление загрузкой - контроль типа файла, размера, количества загружаемых файлов. В принципе, управление загрузкой файлов на сервер можно управлять без подключения дополнительных модулей, напрямую обращаясь к переменной окружения environ['wsgi.input']. Но удобнее воспользоваться модулем cgi. Ниже приведен пример функции, которая анализирует запрос с переданными для загрузки файлами и возвращает информацию об этих файлах. При этом загружать сами файлы пока не будем import cgi def application ( environ , start_response ): formdata = cgi . FieldStorage ( environ = environ , fp = environ [ 'wsgi.input' ]) l = formdata . list output = 'Files count: %d . Size all files: %d \\r\\n ' % ( len ( l ), formdata . length ) # отображаемый размер будет на пару сотен байт больше суммарного размера файлов (это заголовки) for i in xrange ( len ( l )): output += 'Filename: %s , type: %s \\r\\n ' % ( l [ i ] . filename , l [ i ] . type ) status = '200 OK' response_headers = [ ( 'Content-Type' , 'text/plain' ), ( 'Content-Length' , str ( len ( output ))) ] start_response ( status , response_headers ) return [ output ] Проследить как работает скрипт можно, например, воспользовавшись утилитой curl curl -F name1 = @test1.txt -F name2 = @test2.txt http://tests/ul Для сохранения переданных файлов можно применить функцию приведенную ниже. В функцию добавлены следующие типы проверок: * по количеству загружаемых файлов * по размеру каждого загружаемого файла * по типу файла (по его расширению) from tempfile import TemporaryFile import cgi def application ( environ , start_response ): output = upload ( environ ) status = '200 Ok' response_headers = [ ( 'Content-Type' , 'text/plain' ), ( 'Content-Length' , str ( len ( output ))) ] start_response ( status , response_headers ) return [ output ] def upload ( environ ): MAX_COUNT_FILES = 2 MAX_FILE_SIZE = 2024 * 2 EXTENSION = [ 'txt' , 'json' , 'csv' ] formdata = cgi . FieldStorage ( environ = environ , fp = environ [ 'wsgi.input' ]) output = '' files_list = formdata . list count = len ( files_list ) # проверка по количеству if count > MAX_COUNT_FILES : output = ' \\n Attempt upload %d files. Max count - %d . Files not uploaded \\n ' % ( count , MAX_COUNT_FILES ) return output for i in xrange ( count ): # проверка по расширению ext = files_list [ i ] . filename . split ( '.' )[ - 1 ] if ext not in EXTENSION : output += 'File %s has invalid type. Must be %s \\n ' % ( files_list [ i ] . filename , str ( EXTENSION )) continue # проверка по размеру body = files_list [ i ] . file body . seek ( 0 , 2 ) if body . tell () > MAX_FILE_SIZE : output += 'File %s has large size. Max file size - %d \\n ' % ( files_list [ i ] . filename , MAX_FILE_SIZE ) continue body . seek ( 0 ) # сохранение во временный файл с дополнительным контролем размера temp_f = TemporaryFile () m = MAX_FILE_SIZE while m > 0 : part = body . read ( min ( m , 1024 * 200 )) if not part : break temp_f . write ( part ) m -= len ( part ) if part : continue f = open ( '/var/www/tests/upload/' + files_list [ i ] . filename , 'wb' ) temp_f . seek ( 0 ) f . write ( temp_f . read ()) f . close () temp_f . close () output += 'File %s uploaded \\n ' % files_list [ i ] . filename return output","tags":"Инструменты разработки","title":"Apache: управление загрузкой файлов  с помощью wsgi"},{"url":"https://zlukfo.bitbucket.io/articles/apache-autorization","text":"Конечно, эти виды авторизации не самые надежные с точки зрения безопасности. Но они отличаются простотой настройки и вполне применимы для небольших и не слишком секретных проектов. Чтобы разобраться, как работают эти виды авторизации, в чем их преимущества и недостатки, выполним сначала их настройку Настройка авторизации Basic Создаем новый файл (флаг -с) с пользователем admin sudo htpasswd -c /usr/local-conf/.htpasswd admin Вводим и подтверждаем пароль для пользователя 'admin', например 'qwe123'. В каталоге /usr/local-conf/ появится новый файл .htpasswd Подключаем модуль apache, отвечающий за basic-авторизацию sudo a2enmod auth_basic Вы настройках хоста (можно виртуального) добавляем <Directory /var/www/tests/basic> AuthType Basic AuthName \"private\" AuthUserFile /usr/local-conf/.htpasswd Require valid-user </Directory> На что здесь следует обратить внимание: необходимо запретить внешний доступ к чтению файла .htpasswd. Для этого файл следует размещать за пределами каталога сайта. Или в настройках виртуального хоста дополнительно запрещать доступ к нему. Это необходимо, потому что в данных файлах хранится информация о пользователях имеющих доступ в защищенную зону и хэши паролей этих пользователей Перезапускаем apache Digest Создаем новый файл (флаг -с) пароля sudo htdigest -c /usr/local-conf/.htpasswd private admin Обратите внимание: здесь при создании нового пользователя в защищенную зону необходимо вводить псевдоним этой зоны (в отличии от basic) Подключаем модуль apache, отвечающий за digest-авторизацию sudo a2enmod auth_digest В настройках хоста (можно виртуального) добавляем <Directory /var/www/tests/digest> AuthType Digest AuthName private AuthUserFile /usr/local-conf/.htpasswd Require valid-user </Directory> Обратить внимание: здесь обязательно в параметре AuthName нужно указывать псевдоним защищенной зоны указанный нами на шаге 1 Перезапускаем apache Как работает авторизация Basic Когда мы пытаемся зайти на страницу защищенной области (GET-запрос по адресу http://tests/basic/index.html ), происходит следующее Браузер отправляет серверу запрос примерно с таким заголовком GET /basic/ HTTP / 1.1 Host : tests Connection : keep-alive Accept : text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 User-Agent : Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/44.0.2403.89 Chrome/44.0.2403.89 Safari/537.36 HTTPS : 1 Accept-Encoding : gzip, deflate, sdch Accept-Language : ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4 Сервер возвращает ответ HTTP / 1.1 401 Unauthorized Date : Sun, 18 Sep 2016 09:16:25 GMT Server : Apache/2.4.7 (Ubuntu) WWW-Authenticate : Basic realm=\"Private Area\" Content-Length : 451 Keep-Alive : timeout=5, max=100 Connection : Keep-Alive Content-Type : text/html; charset=iso-8859-1 <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"> < html >< head > < title > 401 Unauthorized </ title > </ head >< body > < h1 > Unauthorized </ h1 > < p > This server could not verify that you are authorized to access the document requested. Either you supplied the wrong credentials (e.g., bad password), or your browser doesn't understand how to supply the credentials required. </ p > < hr > < address > Apache/2.4.7 (Ubuntu) Server at tests Port 80 </ address > </ body ></ html > Заметьте, с первым же ответом сервер загрузил страницу с информацией, что авторизация не пройдена. Поэтому если мы отказываемся от авторизации, обмена сообщениями между клиентом и сервером не происходит, браузер просто отображает эту страницу. Если мы вводим неправильный логин-пароль, браузер генерирует и отправляет сообщение примерно со следующим заголовком GET /basic/ HTTP / 1.1 Host : tests Connection : keep-alive Cache-Control : max-age=0 Authorization : Basic YXNtaXY6ZGRk Accept : text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 User-Agent : Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/44.0.2403.89 Chrome/44.0.2403.89 Safari/537.36 HTTPS : 1 Accept-Encoding : gzip, deflate, sdch Accept-Language : ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4 Обратите внимание на ключ Authorization - браузер кодирует (вроде бы) Ваш логин-пароль и тут же сохраняет его у себя. К данному факту мы еще вернемся позже Ответ сервера такой же, что и на шаге 2 Если мы вводим правильный логин-пароль, то получаем от сервера примерно такой ответ HTTP / 1.1 200 OK Date : Sun, 18 Sep 2016 09:44:30 GMT Server : Apache/2.4.7 (Ubuntu) Last-Modified : Sun, 18 Sep 2016 08:31:37 GMT ETag : \"c2-53cc406177934-gzip\" Accept-Ranges : bytes Vary : Accept-Encoding Content-Encoding : gzip Content-Length : 161 Keep-Alive : timeout=5, max=100 Connection : Keep-Alive Content-Type : text/html В теле ответа передается для отображения в браузере загружаемая страница (index.html) И, внимание! Если страница ссылается на другие файлы (скрипты, стили и т.д.), находящиеся внутри защищаемой папки (basic), то браузер автоматически для каждого такого файла отправляет на сервер запросы с заголовками, аналогичными шагу 3. Если подключаемые файлы находятся НЕ в защищаемой области - браузер запрос на проверку пароля для их получения НЕ отправляет Digest Схема авторизации аналогична Basic. Отличие лишь в параметрах, перезаваемых в заголовках запросов и ответов. Ответ сервера о необходимости прохождения авторизации (шаг 2) WWW-Authenticate: Digest realm=\"private\", nonce=\"lz9uTMU8BQA=398ac33371c9922a84e12c06394493152781e575\", algorithm=MD5, qop=\"auth\" Запрос клиента с введенными значениями логина и пароля (шаг 3) Authorization: Digest username=\"admin\", realm=\"private\", nonce=\"bT2bUMU8BQA=c7efe86ce4147b1ada766fbcd96639c521855b45\", uri=\"/digest/\", algorithm=MD5, response=\"bf1ec1979946c684cbe834974be19dc9\", qop=auth, nc=00000001, cnonce=\"896733f4c19f6c9d\" Ответ сервера об успешном прохождении авторизации (шаг 5) Authentication-Info: rspauth=\"c7fa3bfe108be8c47a6fad4b4a0f8125\", cnonce=\"896733f4c19f6c9d\", nc=00000001, qop=auth Теперь разберемся почему Basic и Digest авторизация сегодня считается ненадежной и насколько она ненадежна. Надежность авторизации Basic Прежде всего логин и пароль пользователя хранятся на компьютере пользователя и передаются серверу (шаг 3) БЕЗ шифрования. То что мы видим в заголовке запроса Authorization: Basic YXNtaXY6ZGRk это всего лишь кодирование base64. Преобразовать набор символов в пару логин-пароль не составляет никакого труда, например так https://www.base64decode.org/ Далее Basic авторизация статична - пароль, единожды сохраненный Вашим браузером позволит Вам заходить в последующем в защищенную зону без прохождения авторизации до оконачания сеанса (закрытия браузера) И, наконец, протокол Basic авторизации не предусматривает никаких ограничений на количество попыток авторизации. Digest Работа Digest авторизации чуть сложнее. Прежде всего, при первом сообщении клиенту о необходимости прохождения авторизации (шаг 2), сервер передает в заголовке ряд параметров WWW-Authenticate: Digest realm=\"private\", nonce=\"lz9uTMU8BQA=398ac33371c9922a84e12c06394493152781e575\", algorithm=MD5, qop=\"auth\" realm - имя защищенной области (помните, мы акцентировали на ней внимание при настроке файла конфигурации?) Здесь значение этого параметра играет роль (в отличии от Basic авторизации) nonce - уникальный ключ, генерируемый сервером в момент отправки сообщения клиенту algorithm - название алгоритма (как правило, MD5) Этой информации достаточно браузеру для того чтобы знать как нужно шифровать введенные пользователем логин и пароль перед отправкой на сервер Пользователь вводит логин-пароль, браузер шифрует их и отправляет серверу для проверки следующую информацию (шаг 3) Authorization: Digest username=\"admin\", realm=\"private\", nonce=\"bT2bUMU8BQA=c7efe86ce4147b1ada766fbcd96639c521855b45\", uri=\"/digest/\", algorithm=MD5, response=\"bf1ec1979946c684cbe834974be19dc9\", qop=auth, nc=00000001, cnonce=\"896733f4c19f6c9d\" Здесь прежде всего нужно обратить внимание на параметр response. Это - контрольная сумма с которой будет сверяться сервер. Получив от клиента информацию, сервер берет из файла паролей (.htpasswd) для пользователя username и защищенной зоны realm правильное значение пароля, по известному алгоритму (см. ниже) вычисляет контрольную сумму и сравнивает ее с суммой, переданной в response. Если суммы не совпадают - сервер вновь предлагает пройти авторизацию Алгоритм расчета контрольной суммы (response) одинаков для клиента и сервера. Достаточно понятно от описан здесь Таким образом, главное отличие Digest-авторизация - шифрование пароля. Что, впрочем не до конца исключает уязвимость данного способа авторизации. Итоги Потенциально существует три возможности получения информации о логине и пароле авторизации из данных сохраненных браузером на компьютере клиента из перехваченных запросов клиента серверу из файла .htpasswd, хранящемся на сервере Протоколы авторизации не предусматривают механизм контроля неудачных попыток прохождения авторизации Безопасность На перечисленных недостатках сегодня реализованы различные успешные методы взлома пароля и получения доступа к защищенной зоне сайта. Наиболее распространенными являются подбор пароля и его взлом. Подбор пароля основан на том, что basic и digest авторизация в чистом виде не контролируют количество неудачных попыток авторизации. Поэтому достаточно просто создать программу, автоматически генерирующую запросы доступа в защищенную с подставлением различных паролей (из заранее подготовленного словаря) до тех пор пока один из паролей не подойдет Другой способ - взлом пароля реализуется в два этапа. Сначала необходимо получить информацию о логине-пароле пользователя (из данных сохраненных браузером клиента или перехваченного трафика). Затем из полученной информации извлекается пароль. Если для basic-авторизации это труда не составляет, то с digest-авторизацией сложнее. Но вполне осуществимо. Здесь применяется программное обеспечение использующее различные подходы: прямой подбор паролей из словаря - наименне эффективный метод но при удачно составленном словаре вполне действенный (см пример ниже) использование \"радужных таблиц\" метод \"туннелирования\" используя вычислительные мощности отдельных видов видеокарт Подбор пароля из словаря Можно скачать готовые словари, а можно их сгенерировать самому. Например программой crunch . Пароли генерируются не случайным образом а путем всех возможных перестановок символов, доступных для ввода пароля. Вот один пример crunch 3 6 -f charset.lst mixalpha-numeric-all -o dict3-6.txt Эта команда сгенерирует файл с паролями длиной от 3 до 6 символов с использованием предустановленного словаря mixalpha-numeric-all (все латинские буквы, цифры и нектороные символы). Правда, количество таких паролей будет 697.287.726.760 и занимать на диске они будут около 4 TB. Поэтому, на практите эту утилиту можно использовать только в случае каких либо сведений о пароле, например, что пароль состоит из 8 символов и только из цифр. Так можно попытаться взломать пароль, перебрав 9.000.000 паролей. Полный перебор займет всего 1 минуту # -*- coding: utf-8 -*- import hashlib def calc ( h1 , h2 , nonce , cnonce ): return hashlib . md5 ( h1 + ':' + nonce + ':' + nc + ':' + cnonce + ':' + qop + ':' + h2 ) . hexdigest () if __name__ == '__main__' : #данные о пароле из перехваченного трафика nonce = \"Z97svNM8BQA=71be2e3910ee86fa54a3e4afc13f5b9ecc02303b\" qop = \"auth\" nc = \"00000001\" cnonce = \"ad82cf9261fe82d8\" username = 'admin' realm = 'private' method = 'GET' uri = '/digest/' TEMPLATE_RESP = '9b8b18ff554a7e93f37d3e2b82dc40d0' h2 = hashlib . md5 ( method + ':' + uri ) . hexdigest () f = open ( '9mil.txt' , 'r' ) for passwd in f : h1 = hashlib . md5 ( username + ':' + realm + ':' + passwd [: - 1 ]) . hexdigest () resp = calc ( h1 , h2 , nonce , cnonce ) if resp == TEMPLATE_RESP : print 'Пароль - ' + passwd break Кстати, перехватывать для анализа трафик между клиентом и сервером (локального отладочного) можно примерно так sudo tcpdump -A -s 1024 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)' -i lo Усиление защищенности Basic/Digest авторизации Прежде всего, для себя я не нашел ни одного преимущества Basic перед Digest. Поэтому в дальнейшем буду рассмотривать исключительно способы усиления защищенности Digest авторизации. Будем использовать возможности wsgi. Конфигурацию хоста в части авторизации настраиваем так <Directory \"c:/Apache24/htdocs/test/digest\"> AuthType Digest AuthName \"orivate\" Require valid-user #AuthUserFile \"c:/.ht_digest\" AuthDigestProvider wsgi WSGIAuthUserScript \"c:/Apache24/cgi-bin/auth.wsgi\" </Directory> Теперь прохождением авторизации будет управлять скрипт auth.wsgi. Скрипт должен быть примерно такого содержания import md5 def get_realm_hash ( environ , user , realm ): value = md5 . new () value . update ( ' %s : %s : %s ' % ( user , realm , 'qwe123' )) hash = value . hexdigest () # теперь легко логировать попытки авторизации #f=open('wsgi_auth.log','a') #f.write(str(environ['REMOTE_ADDR'])+'\\t'+user+'\\t'+realm+'\\r') #f.close() if user == 'admin' : return hash return None Ведя логи можно ограничивать попытки авторизации по времени, ip адресу, пользователю и т.д. Что касается защиты пароля от взлома ...","tags":"apache","title":"Apache: Basic/Digest авторизация"},{"url":"https://zlukfo.bitbucket.io/articles/wsgi-router","text":"Попробуем реализовать web-приложение на базе wsgi без использования фреймворков django, flask и пр. Потому, что, зачастую, использование подобных универсальных инструментов в небольших проектах приводит к излишнему усложнению. Исходные положения Структуру web-приложения будем строить исходя из аксиомы, что имеется только одна точка доступа к функционалу приложения. Это скрипт-роутер . Назначение скрипта-роутера Выполняет только следующие задачи: определяет набор комманд web-приложения, доступный пользователям, извлекает из запросов (GET и POST) команды, параметры и их значения, проверяет права пользователей на выполнение команды web-приложения, поступившей в запросе, перенаправляет команду на выполнение соотвествующей функции преобразует результат выполнения функции в единую унифицированную структуру ответа и возвращает его клиенту, отправившему запрос. Такой функционал скрипта-роутера подразумевает определенную организацию всего web-приложения Организация web-приложения Адресация Прежде всего, необходим URL по которому будет запускаться скрипт-роутер. На этапе настройки web-приложения этот путь задается в конфигурации (виртуального хоста). Договоримся, что вызов скрипта-роутера будет выполняться по запросу http://site.name/command . \"command\", в данном случае, это префикс роутер. Далее, запрос должен содержать команду web-приложения, которую хочет выполнить пользователь и ее параметры. Примем следующий синтаксис http://site.name/command/commandName/function/subfunction/../func?p1=n1&p2=n2 ... Т.е. запрос после префикса разбивается на три части: имя команды последовательность вложенных подкоманд набор параметров со значениями Реализация команд web-приложения Команды реализуются через функции, которые подключаются к скрипту-роутеру Из предыдущего пункта естественно вытекает первое требование к таким функциям. Не сильно жесткое. Функция должна принимать три входных параметра: последовательность вложенных подкоманд. Параметр может быть пустым набор параметров функции типа ключ-значение. Данный параметр также может быть пустым. параметр переменных окружения. Часто они бывают нужны для работы функций Второе требование к функции - возвращаемое значение должно принадлежать к одному из типов : 1) Если в результате работы функция возвращает 'отрицательный' ответ, то это - словарь с двумя ключами: - type_message - (\"error\" - текстовое сообщение об ошибке, \"debug\" - отладочная и контрольная информация, ...) - message - данные, соотвествующие значению type_message Данный тип рекомендуется для унификации обработки ответа на стороне клиента Функция может возвращать следующие объекты - число, строка, массив, кортеж, словарь. Корректное вложение этих объектов в ответ для клиента возложена на скрипт-роутер. Словарь, содержащий непосредственно результат работы функции и дополнительные параметры, переопределяющие значения http заголовка ответа по умолчанию. Структура ответа Отправив запрос на выполнение команды web-приложения пользователь ожидает ответа положительного или отрицательного. Поэтому договоримся, что на стороне сервера формируется ответ структуры словаря со сдежующими обязательными ключами '__status' - статус ответа, возвращаемого клиенту (текстовый параметр) '__headers' - заголовки ответа клиенту: массив кортежей из пары (имя параметра заголовка, значение параметра) 'data' - строка возвращаемых данных от прикладной функции В скрипте- роутере определяются значения '__headers' и '__status' по умолчанию, достаточные для передачи ответа пользователю по HTTP. Результат работы команды присваивается ключу 'data' и ответ отправляется пользователю. Если функции, необходимо изменить значение '__headers' или '__status', тогда функция должна возвращать не просто результаты своей работы, но и необходимые значения изменяемых ключей. Безопасность В рамках темы публикации вопрос безопасности web-приложения будем рассматривать только в контексте проверки права пользовательского запроса на выполнение команды web-приложения . Но даже в этом случае не берусь настаивать на какой-либо конкретной реализации проверки и интегрировать ее в скрипт-роутер. Поэтому примем соглашение, что проверка права запроса на выполнение команды выполняется функцией, возвращающей True в случае успеха и False - если проверка права не пройдена. Производительность В проекте Пример wsgi-скрипта, реализующего функции скрипта-роутера в соотвествии с указанными выше требованиями, можно скачать здесь. Пояснения по ключевым вопросам приведены в комментариях скрипта.","tags":"Разработка web-приложений","title":"Один способ организации web-приложений с помощью wsgi"},{"url":"https://zlukfo.bitbucket.io/articles/oauth2","text":"Вводная часть Если при разработке сайта возникает необходимость идентифицировать пользователя, OAuth-авторизация является достаточно простым инструментом в части реализации и защищенным в части безопасности. В публикации рассмотрена OAuth авторизация для социальных сетей Facebook, Vkontakte и аккаунтов поисковых систем Google, Yandex. Этих примеров достаточно для понимания того, что авторизация по OAuth везде одинакова, различия в реализации незначительны. А значит, полученных из публикации знаний будет достаточно для самостоятельной реализации механизма авторизации для любого другого сервиса. Почему бы не использовать готовые библиотеки и скрипты для подключения авторизации к сайту? Потому, что: OAuth авторизация достаточно проста, чтобы разобраться самому и достаточно важна, чтобы не поручать сбор персональных данных посетителей своего сайта незнакомым библиотекам, гораздо быстрее раз и навсегда понять принципы OAuth авторизации, чем разбираться в каждой ошибке сторонней библиотеки при подключении очередного сервиса авторизации. Термины: Все сервисы (социальные сети, поисковые системы, web-сервисы, и пр.) предоставляющие OAuth авторизацию в данной публикации будем называть серверами авторизации Чтобы реализовать на своем сайте OAuth авторизацию нужно: зарегистрировать свой сайт на сервере авторизации. разместить на своем сайте ссылку (кнопку), отправляющую пользователя на страницу сервера авторизации создать серверный скрипт, который будет обрабатывать результаты прохождения авторизации пользователем и, при необходимости, после успешной авторизации сохранит информацию о пользователе на сервере и перенаправит пользователя на нужную страницу сайта. Регистрация сайта на сервере авторизации Для регистрации сайта на сервере авторизации необходимо иметь аккаунт на нем. Например, если Вы хотите организовать для своего сайта OAuth-авторизацию пользователей через Facebook, Вы должны быть зарегистрированы на Facebook. Ссылки на страницы регистрации нашего сайта: Google . На этой странице Вы регистрируете свой сайт, а по ссылке ниже можно получить параметры проекта и настроить вид окна аутентификации. Естественно в конце ссылки вместо id_project нужно указать id Вашего зарегистрированного сайта. https://console.developers.google.com/apis/credentials?id_project Yandex Facebook ВКонтакте После регистрации сайта на сервере авторизации необходимо: запомнить для последующего использования значения параметров 'client_id' , и 'client_secret' ; прописать на сервере авторизации в настройках проекта URI адрес серверного скрипта которому будет передано управление после ввода пользователем пароля на сервере авторизации. для Google адрес прописывается во поле \"Разрешенные URI перенаправления\" Yandex - \"Callback URL\" Facebook - \"URL-адрес сайта\" ВКонтакте - \"Базовый домен\" Переход с сайта на страницу авторизации Как правило, для перехода на страницу сервера авторизации на сайт вешается кнопка, например, так: < button click = \"javascript::location.href='.......'\" ></ button > В качестве значения ссылки указываем адрес сервера авторизации с набором параметров Адреса на страницы авторизации Google: https://accounts.google.com/o/oauth2/auth Yandex: https://oauth.yandex.ru/authorize Facebook: https://www.facebook.com/dialog/oauth ВКонтакте: https://oauth.vk.com/authorize При переходе по указанным ссылкам необходимо передать следующие параметры: response_type=code - (code - значение параметра для нашего случая авторизации) client_id - значение присвоенное сервером авторизации при регистрации сайта redirect_uri - URI адрес нашего серверного скрипта, которому будет передано управление после ввода пользователем пароля на сервере авторизации. Управление передается посредством отправки сервером скрипту GET запроса. state - не обязательный, но полезный параметр. В него помещается произвольное значение. После успешной авторизации пользователем это же значение возвращается серверному скрипту в GET запросе. Например, в качестве значения этого параметра можно указать псевдоним сервера авторизации Это же значение сервер авторизации направит серверному скрипту. Таким образом, скрипт сможет идентифицировать с какого сервера авторизации пришел запрос. scope указывает серверу авторизации какие дополнительные данные пользователя мы запрашиваем для нашего сайта. Пользователь при прохождении авторизации увидит какие его данные хочет получить наш сайт, соглашается с этим или отклоняет авторизацию. Обязательные данные при прохождении автризации будут возвращаться нашему сайту всегда. К обязательным данным прежде всего относится id пользователя на сервере авторизации Из дополнительных параметров о пользователе, скорее всего, понадобится его email. поэтому в параметре scope нужно указать Вконтакте - scope=email Google - scope=https://www.googleapis.com/auth/userinfo.email+https://www.googleapis.com/auth/userinfo.profile Facebook - scope=email Но здесь необходимо помнить, что на Facebook можно зарегистрироваться без указания email, через номер мобильного телефона. Поэтому email у пользователя может не быть. Yandex - параметр scope можно не указывать. Какая информация о пользователе необходима - задается при регистрации приложения Параметры для доступа к другим дополнительным параметрам пользователя можно посмотреть здесь ВКонтакте https://vk.com/dev/auth_sites Facebook - https://developers.facebook.com/docs/facebook-login/permissions#reference-public_profile Обработка результатов авторизации После успешной авторизации пользователя на сервере (отказ от авторизации и неправильный ввод логина-пароля не рассматриваем) нашему скрипту обработки (URI, указанный при регистрации сайта на сервере авторизации и как параметр при переходе пользователя на страницу авторизации) направляется GET-запрос, содержащий параметр 'code' с непустым значением. Получив 'code', скрипт должен добавить его в POST-запрос серверу авторизации на получения access_token. При корректных значениях параметра нашему скрипту возвращается json-объект, содержащий в том числе и нужный acess_token Получив acess_token скрипт отправляет GET-запрос, указав значения acess_token-а в качестве параметра. Если acess_token указан правильно - сервер авторизации возвращает json объект с данными о пользователе. Возвращаемая информация о пользователе имеет формат JSON. Однако, структура JSON, имена ключей у каждого сервера авторизации индивидуальна. Вот и все премудрости OAuth авторизации. Осталось только написать скрипт, обрабатывающий ответы сервера авторизации. Скрипт сделаем на Python Реализация скрипта на python Блок 1 Настройки серверов авторизации service_config = { \"google\" :{ 'code' : \"\" , 'url_token' : \"https://accounts.google.com/o/oauth2/token\" , 'url_user_info' : \"https://www.googleapis.com/oauth2/v1/userinfo\" , 'client_id' : \"....\" , 'client_secret' : \"...\" , 'redirect_uri' : \"http://...\" , 'grant_type' : 'authorization_code' }, \"yandex\" :{ 'code' : '' , 'url_token' : 'https://oauth.yandex.ru/token' , 'url_user_info' : 'https://login.yandex.ru/info' , 'grant_type' : 'authorization_code' , 'client_id' : '...' , 'client_secret' : \"...\" }, \"facebook\" :{ 'code' : '' , 'url_token' : 'https://graph.facebook.com/oauth/access_token' , 'url_user_info' : 'https://graph.facebook.com/me' , 'client_id' : '...' , 'client_secret' : '...' , 'redirect_uri' : \"http://...\" }, \"vkontakte\" :{ 'code' : '' , 'url_token' : 'https://oauth.vk.com/access_token' , 'url_user_info' : 'https://api.vk.com/method/users.get?fields=uid,first_name,last_name,nickname,screen_name,sex,bdate,city,country,timezone,photo' , 'client_id' : '...' , 'client_secret' : '...' , 'redirect_uri' : \"http://...\" } } Для настройки ОАuth авторизации через другой сервер (не приведенный в листинге) - просто добавить соотвествующие настройки Блок 2 Функция прохождения авторизации Помним, что функция запускается, когда пользователь ввел логин-пароль на сервере авторизации и произошел редерикт по адресу, указанному нами при регистрации сайта. По данному адресу был направлен GET запрос, включающий параметры (агрумент вызова функции) 1 import requests 2 def start ( params ): 3 # ШАГ 1 проверяем, заданы ли настройки авторизации для сервиса с которого пришел GET 4 # (см значение state) 5 if not params [ 'state' ] in service_config : 6 return '' 7 8 # ШАГ 2 отсылаем запрос на получение ключа авторизации 9 headers = { 'Content-type' : 'application/x-www-form-urlencoded' } 10 service_config [ params [ 'state' ]][ 'code' ] = params [ 'code' ] 11 r = requests . post ( service_config [ params [ 'state' ]][ 'url_token' ], data = service_config [ params [ 'state' ]], headers = headers ) 12 13 # если неверно заданы параметры приложения в service_config сервер авторизации вернет словарь с ключем error 14 if 'error' in r . text : 15 return 'Параметры приложения заданы неверно' 16 17 # ШАГ 3 отсылаем запрос на получение информации о пользователе 18 if params [ 'state' ] == 'facebook' : 19 # facebook отдает данные не json а в канонической форме - отсюда условие 20 param_token = dict ([ i . split ( '=' ) for i in r . text . split ( '&' )]) 21 # user_id нужно только для ВКонтакте, но мы же делаем универсальный скрипт 22 user_id = '' 23 else : 24 param_token = r . json () 25 user_id = r . json () . get ( 'user_id' , '' ) 26 27 r = requests . get ( service_config [ params [ 'state' ]][ 'url_user_info' ], params = { 'access_token' : param_token [ 'access_token' ], 'user_ids' : user_id , 'format' : 'json' , 'oauth_token' : param_token [ 'access_token' ]}) 28 return r . json () Основная задача выполнена - пользователь идентифицировал себя через ОАuth, нужные данные о нем мы получили. Если не стоит задача куда-либо сохранять эти данные, то на этом можно остановиться. Блок 3 Приведение данных о пользователе к единой форме Нужно учитывать, что информация о пользователе, возвращаемая серверами авторизации имеет разные состав и структуру. Поэтому в общем случае необходимо писать отдельную функцию на каждый сервер авторизации для приведения информации к единому формату. Если на сайте планируется авторизовать пользователей через большое количество серверов, то можно применить такой прием. создаем для каждого сервера авторизации необходимую функцию, обрабатывающую переданную информацию о пользователе. В качестве обязательных параметров функции передаются: param_token - ответ сервера авторизации с access_token (см. выше шаг 2). VKontakte передает адрес почты пользователя в access_token param_info - собственно данные о пользователе, Вот пример функций для рассматриваемых серверов авторизации def getInfoVkontakte ( param_token , param_info ): param = {} param [ 'user_id' ] = str ( param_token [ 'user_id' ]) param [ 'email' ] = param_token [ 'email' ] param [ 'provider' ] = 'vkontakte' param [ 'dop_info' ] = param_info return param def getInfoGoogle ( param_token , param_info ): param = {} param [ 'user_id' ] = param_info [ 'id' ] param [ 'email' ] = param_info [ 'email' ] param [ 'provider' ] = 'google' param [ 'dop_info' ] = param_info return param def getInfoFacebook ( param_token , param_info ): param = {} param [ 'user_id' ] = param_info [ 'id' ] param [ 'email' ] = param_info . get ( 'email' , 'None' ) param [ 'provider' ] = 'facebook' param [ 'dop_info' ] = param_info return param def getInfoYandex ( param_token , param_info ): param = {} param [ 'user_id' ] = param_info [ 'id' ] param [ 'email' ] = param_info [ 'default_email' ] param [ 'provider' ] = 'yandex' param [ 'dop_info' ] = param_info return param Связываем каждую функцию со значением параметра state (см. выше) getInfo = { 'vkontakte' : getInfoVkontakte , 'google' : getInfoGoogle , 'facebook' : getInfoFacebook , 'yandex' : getInfoYandex } В основной функции (блок 2) добавляем строку output = getInfo [ params [ 'state' ]]( param_token , r . json ()) return output Теперь информация о пользователе возвращается в структурированном виде. Ссылки При подготовке публикации были использованы материалы этой статьи Причитать по OAuth можно здесь","tags":"Общие вопросы","title":"Подключаем OAuth авторизацию для своего сайта"},{"url":"https://zlukfo.bitbucket.io/articles/secret-key","text":"Задача Разработать механизм, проверяющий поступивший в web-приложение запрос на право выполнение команды web-приложения. Требования Для пользователя, отправляющего запрос на выполнение команды, проверка прав должна проходить прозрачно - он не должен при каждом запросе вводить пароли для подтверждения своих прав. Это требование подразумевает, что необходимые данные, подтверждающие права пользователя, могут безопасно храниться на локальном компьютере (например, в кукисах), и, по необходимости, отправляться web-приложению автоматически. Общая идея решения Чтобы получить право на выполнение команд web-приложения, пользователь должен сначала авторизоваться (например, по OAuth). После этого web-приложение выделяет ему индивидуальный ключ, который сохраняется в профиле пользователя Действие ключа ограничено . Это делается из соображений безопасности. После того, как действие ключа закончится, пользователю нужно заново запросить ключ у web-приложения (заново пройти авторизацию), чтобы получить новый действующий ключ. Ограничение действия ключа вводится по времени. Время начала действия ключа - немедленное, т.е. ключ начинает действовать сразу после его генерации. Параметр ограничения ключа про времени, имеет несколько единиц детализации. Это - 'минута', 'час', 'сутки'. Такого деления будет достаточно для большинства web-приложений. Генерация ключа и списка контрольных точек Генерация ключа выполняется шифрованием ssh224 входной строки. Входная строка должна содержать: а) идентификаторы пользователя, б) информацию о параметре, ограничивающем действие ключа - время действия ключа. Идентификаторы пользователя - один или более текстовых фрагментов, уникальных только для одного пользователя, например ['user_id', 'user_email'] . Идентификатор пользователя может быть не предусмотрен web-приложением. В этом случае сгенерированный секретный ключ будет одинаковым для всех пользователей Параметр времени, ограничивающие действия ключа - строковое выражение времени генерации ключа. Если web-приложением парметр не предусмотрен, для каждого пользователя будет сгенерирован уникальный бессрочный ключ (на основании его идентификатров). Входная строка для генерации ключа является результатом \"сцепления\" заданных идентификаторов пользователя и времени генерации ключа. Идентификаторы пользователя, время генерации ключа и порядок их \"сцепления\", образующий входную строку для генерации ключа, являются основой защиты описываемого в данной публикации метода валидации пользователя Проверка ключа Проверка действительности ключа проверяется по списку контрольных точек. Если ключ найден в списке - он действителен, если нет - пользователю необходимо получить новый ключ. Формирование списка контрольных точек выполняется сразу после решения web-приложения выделить пользователю секретный ключ, например, после успешного прохождения пользователем OAuth авторизации. Список контрольных точек сохраняется в web-приложении в профиле пользователя. Исходными данными для формирования списка контрольных точек являются а) идентификаторы пользователя, б) текущее время, сразу после успешного прохождения авторизации, в) единица детализации, г) интервал времени ограничивающей время действия ключа, начиная с текущего момента Значение первой контрольной точки в списке генерируется, исходя из текущего времени. Далее, значение времени увеличивается на одну единицу детализации, рассчитывается следующая контрольная точка и добавляется в список контрльных точек. Каждое новое значение параметра ограничения проверяется на принадлежность к заданному интервалу. Как только значение превышает интервал - дальнейшие вычисления прекращаются. Алгоритм валидации пользователя Генерация ключа Право на выполнение команды web-приложения получают только пользователи с сдействующим ключом. Чтобы получить ключ нужно успешно авторизоваться через OAuth. После успешной авторизации web-приложение генерирует список контрольных точек и сохраняет его в профиле пользователя. Генерация ключа для сравнения происходит, когда пользователь отправляет запрос на выполнение операции. Запрос содержит идентификаторы пользователя. Добавляем к нему (в заданном порядке) время поступления запроса и по полученной строке генерируем секретный ключ. Сравниваем ключ со списком контрольных точек: если ключ в списке - запрашиваемая операция выполняется, если нет - web-приложение предлагает пользователю получить новый ключ - пройти авторизацию. Такой алгоритм повышает безопасность, поскольку позволяет не передавать за пределы web-приложения информацию о секретных ключах: список контрольных точек хранится внутри приложения, а секретный ключ вообще нигде не хранится. Если пользователь отправляет web-приложению запрос на выполнение команды без секретного ключа - приложение отправляет его на авторизацию. Дополнительное условие генерации При генерации ключа и списка контрольных точек следует учитывать одну особенность реализации алгоритма. Для того, чтобы обеспечить разумную длину списка контрольных точек и одновременно действительность секретного ключа в течение всего заданного интервала принимается следующее соглашение - время начала генерации секретного ключа и или списка контрольных точек округляется в меньшую сторону до ближайшего целого значения в соотвествии с выбранным масштабом детализации. Реализация Описываемый алгоритм реализован на python в виде класса. Код доступен здесь Создание экземпляра генератора k = KeyGenerator ( time_shift = 2 , salt_position = 0 , text_array = [], time_point = \"hour\" ) time_shift - время действия секретного ключа. Задается целым числом в абстрактрых единицах. Перевод в физические единицы детализации (минута, час, день) определяется значением параметра time_point salt_position - позиция времени генерации ключа в списке идентификаторов пользователя text_array - список ищентификаторов пользователя time_point - масшаб детализации списка контрольных точек Свойства и методы экземпляра класса time_point - свойство экремпляра в котором хранится список временных контрольных точек (в строковом представлении) getEncryptPoints() - метод генерирует и возвращает список контрольных точек createUserKey() - генерирует и возвращает секретный ключ пользователя Пример реализации: Идентификаторами пользователя являются 2 параметра user_id и user_email. После прохождения авторизации ему нужно присвоить секретный ключ, который будет в течение 15 минут подтверждать его право на соверщение авторизации. from secret_key import KeyGenerator #... # user_id и user_email пользователя после успешной авторизации k = KeyGenerator ( 15 , 2 , [ user_email , user_id ], time_point = 'minute' ) key_points = k . getEncryptPoints () # сохранение key_points в профиле пользователя #... # запрос от пользователя на выполнение команды k = KeyGenerator ( 15 , 2 , [ user_email , user_id ], time_point = 'minute' ) secret_key = k . createUserKey () # получение из профиля список контрольных точек key_points и выполняем проверку if secret_key in key_points : # разрешаем выполнение операции # генерируем новый список ключей и сохраняем в профиле пользователя (опционально) else : # направление на авторизацию","tags":"Разработка web-приложений","title":"Валидация пользователя web-приложения"},{"url":"https://zlukfo.bitbucket.io/articles/webix-interface","text":"Webix - удобный фреймворк для создания интерфейсов web-приложения. Дополнительным плюсом является онлайн инструмент для проектирования прототипов. В данной публикации описан один способ, упрощающий создание одностраничного интерфейса web-приложения на базе компонентов Webix. В основу способа заложены следующие идеи: интерфейс имеет две области - статическую и динамическую. В ходе работы с интерфейсом можно переназначать динамическую область. в статической области расположен активный элемент (меню), который, в зависимости от выбранного значения, управляет загрузкой отдельных элементов интерфейса в динамическую область. Активные элементы можно переназначать \"на лету\". после элементов в динамическую область может быть загружен и выполнен js-файл с произвольным кодом. Посмотреть несколько примеров интерфейса web-приложения, соотвествующий указанной концепции можно здесь Структура web-приложения Чтобы реализовать интерфейс, web-приложение должно иметь следующую структуру: index.html - html-страница отображающая интерфейс init.conf - файл параметров интерфейса init.js - скрипт инициализации интерфейса elements - каталог, хранящий файлы с элементами интерфейса codebase - каталог с кодом фреймворка webix, доступный для скачивания , другими js библиотеками и таблицами стилей index.html Главная страница web-приложения. Это обычный html файл, подключающий необходимые для работы скрипты, таблицы стилей, содержащий мета-данные и пр. В обязательном порядке должны подключаться webix.js waitsync.js jquery.js object-traverse.min.js main_webix.css init.js Конечно же, для конкретного web-приложения дополнительно могут быть подключены дополнительные библиотеки и стили. Особенность index.html - тег <body> - пустой. Минимальный код файла index.html будет выглядеть примерно так < html > < head > < META content = \"text/html; charset=utf-8\" http-equiv = \"Content-Type\" > <!-- Библиотеки и стили, обязательные для подключения --> < script type = \"text/javascript\" src = \"codebase/webix.js\" ></ script > < script type = \"text/javascript\" src = \"codebase/waitsync.min.js\" ></ script > < script type = \"text/javascript\" src = \"codebase/jquery-2.1.4.min.js\" ></ script > < script type = \"text/javascript\" src = \"init.js\" ></ script > < script type = \"text/javascript\" src = \"codebase/object-traverse.min.js\" ></ script > < link rel = \"stylesheet\" type = \"text/css\" href = \"codebase/skins/main_webix.css\" > <!-- Дополнительные библиотеки и стили для конкретного проекта --> < script src = \"codebase/jquery.cookie.js\" ></ script > < link rel = \"stylesheet\" type = \"text/css\" href = \"codebase/skins/air.css\" > </ head > < body > </ body > </ html > init.conf Файл конфигурации web-приложения, сохраненный в формате json. Параметрый конфигурации задаются в init.conf только в том случае, если нас по каким-то причинам не устраривают их значения по умолчанию (см. ниже). Поэтому, для многих проектов конфигурацию web-приложения можно вообще не задавать. В этом случае файл init.conf будет выглядеть так - \"{}\". При инициализации web-приложение использует следующие параметры конфигурации: \"elements_path\" - каталог, хранящий файлы с элементами интерфейса, по умолчанию - \"elements\" \"static\" - файл в каталоге \"elements_path\", хранящий элементы интерфейса статической области, по умолчанию - main.webix \"dynamic_area_id\" - id элемента webix, определяющую динамическую область интерфейса, по умолчанию - \"page\" . Сама динамическая область задается в main.webix (см парметр выше). Ведь динамично только содержимое этой области (элементы интерфейса), сама область статична :-). \"action_element\" - id элемента webix, который задан в статической области (файл main.webix) и управляет отображением элементов интерфейса в динамической области, по умолчанию - menu Значения параметров по умолчанию определяются в init.js Дополнительное назначение init.conf При создании проекта может возникнуть необходимость определить некоторые константы с глобальной обрастью видимости. Это можно сделать, задав в файле init.conf дополнительные пары ключ-значение init.js Функция инициализации интерфейса. Запускается при загрузке главной страницы (index.html). Делает следующее: Объявляет глобальные переменную init_conf в которой хранятся параметры конфигурации web-приложения (заданные в init.conf или принятые по умолчанию) Обображает интерфейс web-приложения - статическую и динамическую области Задает функцию Render, которая оботражает элементы интерфейса в динамической области в зависимости от выбранного пункта активного элемента Задает функцию reinit(), которая запускается при необходимости \"на лету\" изменить активный элемент и (или) динамическую область Пошаговый пример создания интерфейса web-приложения Копируем \"болванку\" web-приложения. 1. Создаем прототип интерфейса web-приложения (содержимое файла main.webix) Для создания прототипов интерфейса удобно использовать online редактор http://webix.com/snippet/ При создании интерфейса следует обратить внимание на 3 момента а) Динамическую область интерфейса рекомендуется задать так: { id : 'page' , height : \"auto\" , rows : [{}]} 'page' -значение по умолчанию параметра конфигурации \"dynamic_area_id\" б) в статической области нужно задать активный элемент, управляющий интерфейсом в динамической области. В качестве активного может быть выбран любой элемент webix, допускающий выбор одного значения из нескольких. К таким относятся: menu, sidemenu, combo, tabbar, segmented, select, richselect, radio в качестве id активного элемента нужно указать \"menu\". Если хотим присвоить другое имя - нужно в init.conf затать соотвествующий ключ \"action_element\" в) Самый главный момент! Помимо \"штатных\" параметров, присваиваемых каждому значению активного элемента (таких, как id, value и др.), задаем для каждого значения два дополнительных параметра \"page\" и \"run\": \"page\" - указывает на имя файла (в нотации json) содержащего динамический блок интефейса для данного значения активного элемента. Файл должен быть предварительно создан и хранится в каталоге elements (параметр init.conf->\"elements_path\") \"run\" - содержит имя файла js-скрипта который будет выполнен сразу после загрузки элементов интерфейса в динамическую область. Оба параметра являются не обязательными. Созданный в онлайн редакторе интерфейс сохраняем в каталоге elements под именем main.webix. Помним, что при сохранении должны соблюдаться все правила json-формата. Аналогичным образом создаем и сохраняем блоки интерфейса для динамической области. Ассоциируем каждый файл динамического блока интерфейса с одним из значений активного элемента (см. п. 1-в) (Опционально) При необходимости создаем js-файл, который ассоциируем с одним из значений активного элемента (см. п. 1-в) Данный js-файл будет выполнен сразу после загрузки элементов интерфейса в динамическую область. Приемы применения Как изменить активный элемент и(или) динамическую область \"на лету\"? Для этого, в js-коде нужно изменить параметры init_conf['active_element'] и (или) init_conf['dynamic_area_id'] и вызвать функцию reinit(). Например, мы планируем сделать элемент 'elem1' активным при получении им фокуса. для этого пишем примерно следующий код: $$ ( \"elem1\" ). attachEvent ( \"onFocus\" , function (){ init_conf [ 'active_element' ] = 'submenu_one' reinit () })","tags":"Разработка web-приложений","title":"Webix: упрощаем разработку интерфейса web-приложения"},{"url":"https://zlukfo.bitbucket.io/articles/sublimetext-environ","text":"Редактирование файлов на удаленном хосте Зачастую удаленный хост - это арендуемый VDS. Вариантов решения задачи несколько, привожу наиболее понравившийся - редактирование файлов по SSH. Устанавливаем в Sublime Text плагин rsub На удаленном хосте выполняем следующие действия curl https://raw.github.com/aurora/rmate/master/rmate > rmate sudo mv rmate /usr/local/bin sudo chmod +x /usr/local/bin/rmate подключаемся к удаленному хосту по SSH ssh -R 52698:localhost:52698 user@host На этом настройка завершена. Теперь чтобы отредактировать файл, на локальном компьютере запускаем Sublime Text а на удаленном хосте вводим rmate /path/to/file Для большего удобства (субъективно) на указанное выше действие в midnight commander можно настроить пользовательское меню. Как сделать - читайте в этой публикации. Источники: https://habrahabr.ru/post/181299/","tags":"Инструменты разработки","title":"Настройка Sublime Text для комфортной работы"},{"url":"https://zlukfo.bitbucket.io/articles/ident-word","text":"Необходимость идентификации слов поискового запроса непосредственно вытекает из задачи поиска по базе текстов. Чтобы не запутаться, в данной публикации будем использовать некоторые термины в следующих контекстах: информационная база – множество текстов, в которых бы будет выполнять поиск поисковый запрос – слово или словосочетание, которые будут искаться в информационной базе эффективность поиска – доля найденных текстов (фрагментов текста) соответствующих поисковому запросу в общей массе текстов информационной базы. поисковая система – совокупность программных средств, которая выполняет поиск в информационной базе. Включает логику поиска, которая определяет эффективность поиска. Цель поиска – найти тексты (фрагменты текстов) из информационной базы, которые максимально точно соответствуют информационному смыслу поискового запроса. Исходя из цели, поиск \"как есть\", т.е. с таким же написанием слов и их порядком в словосочетаниях наверняка даст низкую эффективность, хотя бы потому, что: 1) слово может быть написано с ошибкой – в этом случае оно не будет найдено. 2) поскольку целью является поиск близких по смыслу текстов (фрагментов), необходимо также учитывать все формы слов поискового запроса, а сделать это при поиске \"как есть\" невозможно. Поэтому поисковые системы применяют другой подход. Прежде всего для поисковой системы задается словарь значимых слов – такие слова, которые с учетом их взаимного расположение в предложении полностью сохраняют информационный смысл предложения. Слова, не попавшие в словарь считаются незначимыми, при поиске не учитываются. На этапе задания словаря необходимо решить следующие задачи определить, какие слова считать значимыми и включить их в словарь. Общий подход к решению задачи такой – либо используют уже существующие авторитетные словари общей лексики для каждого языка (например, для русского языка – это словарь Зализняка). Либо, если тематика текстов информационной базы достаточно специфична и имеет много не общеупотребимых слов (например, профессиональных терминов), отсутствующих в словарях общей лексики, то сначала из основе информационной базы (ее репрезентативной выборки) создают частотный словарь содержащихся в ней слов, уже на основании которого выделяют значимые слова и, например, дополняют ими существующий словарь. При анализе частотного словаря слов следует учитывать следующее 1) слова, которые встречаются практически во всех текстах не должны включаться в список значимых слов (поскольку они никак уникально не выделяют текст в котором содержатся) – это стоп-слова, их наоборот нужно исключать при поиске. К стоп-словам также относятся предлоги и союзы. Редкие слова тоже не имеет смысла включать в словарь значимых слов. Кроме того, на данном этапе необходимо, как правило, позаботиться о синонимах, аббревиатура, тезаурусе 2. Если слово считается значимым, то и все его формы также считаются значимыми и должны быть включены в словарь. Однако, это увеличивает размер словаря, а значит 1) будет замедлять скорость поиска и 2) различные формы слова будут идентифицироваться по-разному, хотя имеют одинаковый информационный смысл. Как лучше всего учитывать словоформы? Один из выходов – использовать нормальную форму слова и правила образования слов. Очевидно, что понятие \"нормальная форма слова\" и и правила образования словоформ для каждого языка разные. Например, для русского языка нормальная форма слова а) для существительных – именительный падеж единственного числа, б) для прилагательных – именительный падеж единственного числа мужского рода, в) для глаголов, причастий и деепричастий – глагол в инфинитиве. В настоящее время реализовано несколько вариантов синтаксиса описания правил образования слов для любого языка. Один из наиболее распространенных- ispell. Этот синтаксис достаточно компактен, словари ispell разработаны для многих языков и есть в свободном доступе для скачивания, поэтому, возможно, являются достаточно хорошим решением для задачи образования словоформ. Итак, создав словарь значимых слов, добавив в него (по необходимости) специальную терминологию, аббревиатуры, тезаурус и имея правила образования словоформ. можно эффективно идентифицировать слова поискового запроса… Если только слова в поисковом запросе или текстах информационной базы написаны без ошибок 3. Но ошибки встречаются достаточно часто, значит необходимо распознавать слова с учетом ошибок. Принципиальных решений данной задачи два, эффективность использования того или иного зависит от типа допущенной ошибки. а) применение одного из алгоритмов (Леванштейна, триграммного индекса, …) для нахождения самого близкого по написанию слова из словаря к исходному слову, написанному с ошибкой. Данный способ не привязан к какому -либо языку и конкретному словарю, однако, достаточно часто приводит к неопределенности выбора самого схожего слова, разрешить которую в автоматическом режиме достаточно сложно. б) использование эвристических правил поиска и корректировки ошибок. Алгоритмы данного способа наоборот используют закономерности допускаемых ошибок в словах конкретного словаря. Это позволяет им более точно распознавать допущенные ошибки Идея: на основе существующих общесловарных словарей ispell разработать инструмент идентификации слов поисковой фразы с учетом грамматических ошибок. Реализация идеи описана здесь, пример реализации – здесь В данной публикации предложен один из вариантов идентификации слов при анализе текстов с помощью Python. Задача : проверить, принадлежит ли заданное слово одному или нескольким имеющихся в нашем распоряжении словарям. Или же это слово, которого нет ни в одном из словарей? И, естественно, мы не знаем на каком языке будет написано проверяемое слово (предполагаем только, что на одном из наиболее востребованных). Данное ограничение определило главное требование к задаче – у нас должны быть словари (бесплатные) для различных, наиболее востребованных языках и простая возможность их подключения для поиска. Выбор оказался невелик – ispell, со всеми его преимуществами: свободный доступ для скачивания словарей для нескольких десятков языков сочетание значительного словарного запаса и компактность хранения. Например, словарь русского языка содержит более 120 тыс. уникальных форм, позволяющих генерировать более 4,1 млн. словоформ, и одновременно с этим занимает 1,6 МБ в распакованном виде. Структура словаря позволяет выполнять идентификацию слова по всем словоформам О формате словаря ispell можно узнать отсюда Осталось только проверить правильность нашего выбора. Для этого был написан вот этот скрипт. Все возможности работы со словарями реализованы в виде методов класса: загрузка словаря для любого языка, составленного в синтаксисе ispell. Конвертация словаря базу типа {‘key': ‘value'} в один из форматов по выбору ‘shelve' или ‘cdb' (для формата cdb необходимо подключение библиотек ‘cdb' и simplejson) my_dict = Dict ( 'ru_RU.dic' , 'ru_RU.aff' ) my_dict . toShelve ( 'russian' ) my_dict = Dict ( 'en_US.dic' , 'en_US.aff' ) my_dict . toCdb ( 'english' ) Поиск слова по одному или нескольким словарям my_dict = Dict ( dict_name = 'russian' , db_type = 'shelve' ) if __name__ == \"__main__\" : myDict = Dict ( dict_name = 'russian' , db_type = 'cdb' ) myDict . addDict ( dict_name = 'english' , db_type = 'shelve' ) myDict . addDict ( dict_name = 'spain' , db_type = 'cdb' ) words = [ u'абонированного' , u'papering' , u'abarañar' ] for word in words : word = myDict . findWord ( word ) print 'word:' , word [ 0 ][ 'word' ], '; dict:' , word [ 0 ][ 'dict' ] Тестирование эффективности проводилось по трем показателям скорость работы скрипта объем занимаемой оперативной памяти Для оценки использования словарей ispell как инструмента идентификации в качестве исходного текста был взят Код скрипта можно скачать на Github там.же – описание структуры базы","tags":"Анализ текста","title":"Идентификация слов"},{"url":"https://zlukfo.bitbucket.io/articles/ispell-format","text":"Ispell - это программа проверки правильности написания слов (орфографии). Но интересна не сама программа, а словари, с помощью которых проверяется орфография. Во-первых, словари на основе которых работает ispell созданы для нескольких десятков языков и наречий. И они находятся в свободном доступе Во-вторых - они легко могут быть применены для организации полнотекстового поиска в базах данных. В третьих... В третьих - формат словаря позволяет в компактной форме хранить правила образования форм слова. Словарь состоит из двух текстовых файлов – файла слов и файла аффиксов (правил образования форм слова) Файл слов . Состоит из уникальных слов в \"исходной\" форме и ключей – ссылок на правила словообразования из исходной формы. Каждое слово располагается на отдельной строке. Ключи отделяются от слова символом ‘/' Файл аффиксов . Содержит правила словообразования и другую информацию о словаре. Информация хранится в виде \"имя_параметра\" \"значение_параметра\" (значение отделено от имени параметра пробелами). Подробная информация о всех параметрах, используемых в файле аффиксов приведена здесь . Однако, в доступных для скачивания словарях используются далеко не все параметры. Наиболее важными являются \"SET\" значение параметра указывает на кодировку, в которой сохранен файл словаря. Например, \"KOI8-R\" или \"ISO8859-1\" \"TRY\" значение содержит строчные и заглавные буквы алфавита, используемые для написания слов словаря \"SFX\" - значения параметра определяют правило словообразования путем преобразования суффиксной части слова (расположенной в конце слова) \"PFX\" - значения параметра определяют правило словообразования путем преобразования префиксной части слова (расположенной вначале слова) Последние два параметра являются наиболее важными, их значения имеют одинаковую структуру. Структура значения параметра \"SFX\" (\"PFX\") Значение параметра состоит из трех или четырех полей. Поля отделяются друг от друга пробелами. Если значение параметра состоит из трех полей и 3-е поле цифра N, это означает, что следующие N строк являются блоком правил словообразования. Ключ данного блока указан в 1-м поле, 2-е поле может принимать значение \"Y\" или \"N\", разрешая или запрещая соединение префиксов и суффиксов. SFX L Y 34 Каждое правило словообразования состоит из ключа \"SFX\" (\"PFX\") и его значения из четырех полей 1-е поле – повторяется ключ, указанный в блоке правил 2-е поле – суффиксное окончание слова, которое будет ИСКЛЮЧАТЬСЯ из исходной формы слова при словообразовании по данному правилу. В поле может быть указана цифра 0 – значит из исходной формы слова по данному правилу ничего не исключается 3-е поле – суффиксное окончание слова, которое будет ДОБАВЛЯТЬСЯ в исходную форму слова вместо исключенной . В поле может быть указана цифра 0 – значит в исходную форму слова по данному правилу ничего не добавляется 4-е поле – суффиксное окончание, которое определяет подмножество слов в исходной форме, к которым применимо данное правило. Чтобы данное плавило было применимо к исходному слову – оно должно заканчиваться последовательностью, указанной в поле 4. Данное поле может включать элементы синтаксиса регулярных выражений – ‘[]', ‘&#94;', '.' SFX L o erнas [bdhjlnсpst]o","tags":"Анализ текста","title":"Формат словарей ispell"},{"url":"https://zlukfo.bitbucket.io/articles/python-pelican","text":"Введение Pelican это библиотека на python, которая очень просто решает задачу генерации статических сайтов. Соотвественно, эта статья для тех, кто: знает, что python это язык программирования, в состоянии установить его и библиотеку Pelican на компьютер, имеет о чем рассказать миру, т.е. хочет вести собственный блог, но почему-то не удовлетворен рамками Facebook, Twitter и пр., стремится к простоте, а значит понимает, что если нечто имеет неприятное по звучанию название, например Drupal, Joomla, Wordpress, ... то понятным и управляемым это нечто быть не может. А значит рано или поздно обязательно познакомит Вас с массой проблем различной сложности. Цель публикации - показать, что создание сайта (типа личный блог) с помощью Pelican это просто. Но поскольку любоваться собственным сайтом в одиночку не очень интересно, в данной публикации по шагам рассмотрен весь жизненный цикл разработки - от подготовки проекта до размещения сайта в интернете. Шаг 0. Предполагается , что на Вашем компьютере уже установлены python и Pelican. Если нет - вот ссылки на дистрибутивы и инструкции по установке для Python и для Pelican Шаг 1. Создание проекта сайта В самом простом варианте - на диске нужно создать каталог в котором будет хранится содержимое проекта нашего сайта и запустить в каталоге консольную команду быстрой установки: pelican-quickstart С опытом станет очевидно, что такая организация проекта влечет некоторые неудобства при его сопровождении. Чтобы этого избежать, рекомендую прочитать эту статью Запущенная программа в диалоговом режиме попросит Вас задать некоторые параметры проекта. Что они означают, можно посмотреть здесь . В принципе, можно не заморачиваться и на все вопросы где предлагается выбор да/нет применить значения по умолчанию (просто нажимать Enter). После завершения работы установщика в каталоге сайта будет создано несколько файлов и папок. Сейчас нас интересует только одна папка - content . В нее мы будем сохранять контент (тексты статей и изобращения) нашего сайта. Шаг 2. Написание статей для сайта С точки зрения Pelican, статья сайта - это обычный текстовый файл, содержащий 1) текст и 2) команды управления, отвечающие за внешний вид статьи на странице сайта. Поэтому, публикации можно писать в любом текстовом редакторе, например, Sublime Text . Далее, поясним так. Команды управления в статье могут быть одного из двух форматов restructuredtext: или markdown: . Разница между ними небольшая, далее в статье будет рассмотриваться формат restructuredtext (просто потому, что больше нравится). Соотвественно файлы публикаций для нашего сайта мы должны сохранять в каталоге проекта content как текстовые файлы с расширением rst . Каждая статья размещается в отдельном файле. Файл каждой статьи должен начинаться примерно с такого блока Название статьи ############### :tags: python, pelican :category: Инструменты разработки :slug: python-pelican :summary: Основы работы c генератором статических сайтов pelican :date: 2016-05-24 Текст статьи .... В принципе обязательным является только \":date:\". Остальные команды включаете в статью по своему желанию. \":slug:\" - URL статьи. \":tag:\" и \":category:\" - помогут генератору сайта правильно сгруппировать статьи и быстро находить их на сайте Формат restructuredtext очень гибкий и позволяет вставлять в текст статьи изображения, формулы, блоки кода, внутренние и внешние ссылки и многое другое. Подробнее ознакомиться с возможностями формата можно на этом сайте: Шаг 3. Генерация и просмотр сайта Итак, Вы создали первую статью и сохранили ее в папке content . Чтобы посмотреть как будет выглядеть сайт нужно из каталога проекта выполнить две консольные команды. Запуск генератора сайта pelican content -s publishconf.py Сгенерирированные страницы сайта будут находиться в каталоге output Вашего проекта. Запуск локального сервера cd output python -m SimpleHTTPServer После этого наш сайт станет доступен в браузере по адресу http://localhost:8000 Шаг 4. Размещение сайта в интернете Самый простой способ размещения полученного сайта в интернете - закачать его на один из git репозиториев, например Github, Bitbucket. Как это сделать, можно прочитать здесь. Итог В отведенное время (10 минут) уложились - блог создан и опубликован в интернете. Однако, чтобы его посещали кто-нибудь кроме Ваших родных и друзей, необходимо учесть еще 2 \"мелочи\": контент и дизайн. Вопрос раскрутки сайта, который также достаточно сильно влияет на его посещаемость - это \"алхимия\", которая выходит за тематику этого блога. Создавать нормальный контент для сайта Вас вряд ли кто-то научит. А вот как настроить дизайн своего сайта и удобно добавлять в него (править) статьи - читайте здесь.","tags":"Инструменты разработки","title":"Знакомство с Pelican: блог в интернете за 10 минут"},{"url":"https://zlukfo.bitbucket.io/articles/python-pelican-extend","text":"В статье приведен один способ организации проекта Pelican, упрощающий его сопровождение (добавление контента на сайт, изменение дизайна, добавление плагинов и пр.). Есть и другие способы, какой будете использовать Вы - дело вкуса. Структура проекта В каталоге проекта myProject создадим два подкаталога myProject/ pelican - здесь будет развернут проект Pelican со всеми настройками и контентом myProject/ output - сюда будут сохраняться сгенерированные страницы сайта. В директории pelican разворачиваем новый проект cd pelican pelican-quickstart Создаем каталоги - myProject/pelican/ pelican-plugins - здесь будут храниться нужные для проекты плагина - myProject/pelican/ pelican-themes - каталог для хранения тем сайта В каталоге проекта создаем два командных файла 1. myProject/pelican/gen.bat pelican -r -o ../output для отслеживания изменений контента в my_project/pelican/content в фоновом режиме. Если Вы изменили статью и сохранили изменения, выполнится автоматическая генерация обновленных страниц сайта, котореы сохранятся в каталоге myProject/output 2. myProject/pelican/local_server.bat cd ../output python -m CGIHTTPServer данный файл запустит локальный http-сервер. Сгенерированный сайт будет доступен в браузере по адресу http://localhost:8000 Чего добились: приведенная структура разделила проект Pelican и сгенерированный сайт. Git репозитории, созданные отдельно в каталогах output и pelican упростят публикацию сайта и хранение проекта. запуск командных файлов упрощает просмотр обновлений на сайте после изменения контента. Конфигурация проекта Существует основной файл конфигурации - myProject/pelican/ pelicanconf.py . В нем размещены параметры, касающиеся всего проекта. Назовем их условно \"глобальными\" параметрами проекта. К таким, например относится место расположения статических файлов (css, js, изображений), формат генерации ссылок на страницы сайта и т.д. Ниже приведены некоторые \"глобальные\" параметры конфигурации, которые полезно будет задать для большинства проектов Pelican THEME = 'pelican-themes \\\\ gym' ARTICLE_URL = 'articles/{slug}' ARTICLE_SAVE_AS = 'articles/{slug}/index.html' STATIC_PATHS = [ 'images' , 'static' ] PLUGIN_PATH = 'pelican-plugins' PLUGINS = [ 'sitemap' , 'extract_toc' , 'tipue_search' ] SITEURL = \"http://localhost:8000\" DELETE_OUTPUT_DIRECTORY = True DEFAULT_METADATA = { 'date' : u'2016-05-24' , 'status' : 'published' } Параметры, зависящие от выбранной темы сайта, или влияющие на работу конкретного js скрипта (назовем их \"локальными\" параметрами проекта) лучше разместить в отдельном файле конфигурации. Для этого в каталоге проекта создаем файл myProject/pelican/ myconf.py и задаем в нем локальные параметры. Чтобы генератор сайта \"подхватывал\" параметры конфигурации из обоих файлов, в pelicanconf.py добавим строки from __future__ import unicode_literals import os , sys sys . path . append ( os . getcwd ()) from myconf import * Чего добились: упростили код файла конфигурации","tags":"Инструменты разработки","title":"Python Pelican: настройка проекта для комфортной работы"}]}